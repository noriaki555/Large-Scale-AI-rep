{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPPqde8Sc/Uc5ag8TqfyOvV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["5.2感情分析モデルの実装\n"],"metadata":{"id":"iMlCEeoQnpRg"}},{"cell_type":"code","source":["!pip install transformers[ja,torch] datasets matplotlib japanize-matplotlib\n","\n","from transformers.trainer_utils import set_seed\n","\n","# 乱数シードを42に固定\n","set_seed(42)\n","\n","from pprint import pprint\n","from datasets import load_dataset\n","\n","# Hugging Face Hub上のllm-book/wrime-sentimentのリポジトリから\n","# データを読み込む\n","train_dataset = load_dataset(\"llm-book/wrime-sentiment\", split=\"train\")\n","valid_dataset = load_dataset(\"llm-book/wrime-sentiment\", split=\"validation\")\n","# pprintで見やすく表示する\n","pprint(train_dataset[0])\n","\n","from transformers import AutoTokenizer\n","\n","# Hugging Face Hub上のモデル名を指定\n","model_name = \"cl-tohoku/bert-base-japanese-v3\"\n","# モデル名からトークナイザを読み込む\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","# トークナイザのクラス名を確認\n","print(type(tokenizer).__name__)\n","\n","pprint(encoded_input)\n","\n","tokenizer.convert_ids_to_tokens(encoded_input[\"input_ids\"])"],"metadata":{"id":"xyc3-yl6oA8e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5.2.4 データセット統計の可視化"],"metadata":{"id":"XtLsFjT6qdDm"}},{"cell_type":"code","source":["from collections import Counter\n","import japanize_matplotlib\n","import matplotlib.pyplot as plt\n","from datasets import Dataset\n","from tqdm import tqdm\n","\n","plt.rcParams[\"font.size\"] = 18  # 文字サイズを大きくする\n","\n","def visualize_text_length(dataset: Dataset):\n","    \"\"\"データセット中のテキストのトークン数の分布をグラフとして描画\"\"\"\n","    # データセット中のテキストの長さを数える\n","    length_counter = Counter()\n","    for data in tqdm(dataset):\n","        length = len(tokenizer.tokenize(data[\"sentence\"]))\n","        length_counter[length] += 1\n","    # length_counterの値から棒グラフを描画する\n","    plt.bar(length_counter.keys(), length_counter.values(), width=1.0)\n","    plt.xlabel(\"トークン数\")\n","    plt.ylabel(\"事例数\")\n","    plt.show()\n","\n","visualize_text_length(train_dataset)\n","visualize_text_length(valid_dataset)\n","\n","for data in valid_dataset:\n","  if len(tokenizer.tokenize(data[\"sentence\"]))<10:\n","    pprint(data)\n","\n","def visualize_labels(dataset: Dataset):\n","    \"\"\"データセット中のラベル分布をグラフとして描画\"\"\"\n","    # データセット中のラベルの数を数える\n","    label_counter = Counter()\n","    for data in dataset:\n","        label_id = data[\"label\"]\n","        label_name = dataset.features[\"label\"].names[label_id]\n","        label_counter[label_name] += 1\n","    # label_counterを棒グラフとして描画する\n","    plt.bar(label_counter.keys(), label_counter.values(), width=1.0)\n","    plt.xlabel(\"ラベル\")\n","    plt.ylabel(\"事例数\")\n","    plt.show()\n","\n","visualize_labels(train_dataset)\n","visualize_labels(valid_dataset)\n"],"metadata":{"id":"b5H4_7HWq-0x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5.2.5 データセットの前処理"],"metadata":{"id":"972MQ8pStUwy"}},{"cell_type":"code","source":["from transformers import BatchEncoding\n","\n","def preprocess_text_classification(\n","    example: dict[str, str | int]\n",") -> BatchEncoding:\n","    \"\"\"文書分類の事例のテキストをトークナイズし、IDに変換\"\"\"\n","    encoded_example = tokenizer(example[\"sentence\"], max_length=512)\n","    # モデルの入力引数である\"labels\"をキーとして格納する\n","    encoded_example[\"labels\"] = example[\"label\"]\n","    return encoded_example\n","\n","    encoded_train_dataset = train_dataset.map(\n","    preprocess_text_classification,\n","    remove_columns=train_dataset.column_names,\n",")\n","encoded_valid_dataset = valid_dataset.map(\n","    preprocess_text_classification,\n","    remove_columns=valid_dataset.column_names,\n",")\n","\n","print(encoded_train_dataset[0])"],"metadata":{"id":"-GwNVeJateSH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5.2.6 ミニバッチ構築"],"metadata":{"id":"4pNn2-yGuLpy"}},{"cell_type":"code","source":["from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","batch_inputs = data_collator(encoded_train_dataset[0:4])\n","pprint({name: tensor.size() for name, tensor in batch_inputs.items()})"],"metadata":{"id":"MtvVoK9ruUmP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5.2.7 モデルの準備"],"metadata":{"id":"BrCQHC7cuxvi"}},{"cell_type":"code","source":["from transformers import AutoModelForSequenceClassification\n","\n","class_label = train_dataset.features[\"label\"]\n","label2id = {label: id for id, label in enumerate(class_label.names)}\n","id2label = {id: label for id, label in enumerate(class_label.names)}\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    model_name,\n","    num_labels=class_label.num_classes,\n","    label2id=label2id,  # ラベル名からIDへの対応を指定\n","    id2label=id2label,  # IDからラベル名への対応を指定\n",")\n","print(type(model).__name__)\n","\n","print(model.forward(**data_collator(encoded_train_dataset[0:4])))"],"metadata":{"id":"bICtPbovu6qY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5.2.8 訓練の実行"],"metadata":{"id":"R-tgkWdiva8X"}},{"cell_type":"code","source":["from transformers import TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir=\"output_wrime\",  # 結果の保存フォルダ\n","    per_device_train_batch_size=32,  # 訓練時のバッチサイズ\n","    per_device_eval_batch_size=32,  # 評価時のバッチサイズ\n","    learning_rate=2e-5,  # 学習率\n","    lr_scheduler_type=\"linear\",  # 学習率スケジューラの種類\n","    warmup_ratio=0.1,  # 学習率のウォームアップの長さを指定\n","    num_train_epochs=3,  # エポック数\n","    save_strategy=\"epoch\",  # チェックポイントの保存タイミング\n","    logging_strategy=\"epoch\",  # ロギングのタイミング\n","    evaluation_strategy=\"epoch\",  # 検証セットによる評価のタイミング\n","    load_best_model_at_end=True,  # 訓練後に開発セットで最良のモデルをロード\n","    metric_for_best_model=\"accuracy\",  # 最良のモデルを決定する評価指標\n","    fp16=True,  # 自動混合精度演算の有効化\n",")\n","\n","import numpy as np\n","\n","def compute_accuracy(\n","    eval_pred: tuple[np.ndarray, np.ndarray]\n",") -> dict[str, float]:\n","    \"\"\"予測ラベルと正解ラベルから正解率を計算\"\"\"\n","    predictions, labels = eval_pred\n","    # predictionsは各ラベルについてのスコア\n","    # 最もスコアの高いインデックスを予測ラベルとする\n","    predictions = np.argmax(predictions, axis=1)\n","    return {\"accuracy\": (predictions == labels).mean()}\n","\n","    from transformers import Trainer\n","\n","trainer = Trainer(\n","    model=model,\n","    train_dataset=encoded_train_dataset,\n","    eval_dataset=encoded_valid_dataset,\n","    data_collator=data_collator,\n","    args=training_args,\n","    compute_metrics=compute_accuracy,\n",")\n","trainer.train()"],"metadata":{"id":"44llPVC6vnnk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5.2.9 訓練後のモデル評価"],"metadata":{"id":"EqbFBuppwai9"}},{"cell_type":"code","source":["# 検証セットでモデルを評価\n","eval_metrics = trainer.evaluate(encoded_valid_dataset)\n","pprint(eval_metrics)"],"metadata":{"id":"8fb6xTFtwf38"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5.2.10 モデルの保存"],"metadata":{"id":"VEEK_30ow3Qr"}},{"cell_type":"code","source":["# Googleドライブをマウントする\n","from google.colab import drive\n","\n","drive.mount(\"drive\")\n","\n","# 保存されたモデルをGoogleドライブのフォルダにコピーする\n","!mkdir -p drive/MyDrive/llm-book\n","!cp -r output_wrime drive/MyDrive/llm-book\n","\n","from huggingface_hub import login\n","\n","login()\n","\n","# Hugging Face Hubのリポジトリ名\n","# \"YOUR-ACCOUNT\"は自らのユーザ名に置き換えてください\n","repo_name = \"YOUR-ACCOUNT/bert-base-japanese-v3-wrime-sentiment\"\n","# トークナイザとモデルをアップロード\n","tokenizer.push_to_hub(repo_name)\n","model.push_to_hub(repo_name)"],"metadata":{"id":"Qn6fNybOxPQy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5.3 感情分析モデルのエラー分析"],"metadata":{"id":"I1jRoEELyYdO"}},{"cell_type":"markdown","source":["・ダウンロードコードによる\n"],"metadata":{"id":"2i_jvjg6vQ2_"}},{"cell_type":"code","source":["!pip install datasets transformers[ja,torch] matplotlib scikit-learn\n","\n","from transformers import pipeline\n","\n","model_name = \"llm-book/bert-base-japanese-v3-wrime-sentiment\"\n","sentiment_pipeline = pipeline(model=model_name, device=\"cuda:0\")\n","\n","from datasets import load_dataset\n","\n","valid_dataset = load_dataset(\"llm-book/wrime-sentiment\", split=\"validation\")\n","\n","from tqdm import tqdm\n","\n","# ラベル名の情報を取得するためのClassLabelインスタンス\n","class_label = valid_dataset.features[\"label\"]\n","\n","results: list[dict[str, float | str]] = []\n","for i, example in tqdm(enumerate(valid_dataset)):\n","    # モデルの予測結果を取得\n","    model_prediction = sentiment_pipeline(example[\"sentence\"])[0]\n","    # 正解のラベルIDをラベル名に変換\n","    true_label = class_label.int2str(example[\"label\"])\n","\n","    # resultsに分析に必要な情報を格納\n","    results.append(\n","        {\n","            \"example_id\": i,\n","            \"pred_prob\": model_prediction[\"score\"],\n","            \"pred_label\": model_prediction[\"label\"],\n","            \"true_label\": true_label,\n","        }\n","    )"],"metadata":{"id":"u4ddRoX6yi5g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["・本記載のコードによる"],"metadata":{"id":"gqRVo-STxdQo"}},{"cell_type":"code","source":["!pip install datasets transformers[ja,torch] matplotlib scikit-learn\n","\n","from transformers import pipeline\n","\n","model_name = \"llm-book/bert-base-japanese-v3-wrime-sentiment\"\n","marc_ja_pipeline = pipeline(model=model_name, device=\"cuda:0\")\n","\n","#Google ドライブをマウントする\n","from google.colab import drive\n","\n","drive.mount(\"drive\")\n","\n","!cp -r druve/MyDrive/llm-book/output_marc_ja/last ./\n","\n","from transformers import pipeline\n","\n","model_save_path=\"last\"\n","\n","marc_ja_pipeline=pipeline(model=model_save_path,device=\"cuda:0\")\n","\n","from datasets import load_dataset\n","\n","valid_dataset = load_dataset(\"llm-book/JGLUE\",name=\"MARC-ja\" split=\"validation\")\n","\n","from tqdm import tqdm\n","\n","# ラベル名の情報を取得するためのClassLabelインスタンス\n","class_label = valid_dataset.features[\"label\"]\n","\n","results: list[dict[str, float | str]] = []\n","for i, example in tqdm(enumerate(valid_dataset)):\n","    # モデルの予測結果を取得\n","    model_prediction = marc_ja_pipeline(example[\"sentence\"])[0]\n","    # 正解のラベルIDをラベル名に変換\n","    true_label = class_label.int2str(example[\"label\"])\n","\n","    # resultsに分析に必要な情報を格納\n","    results.append(\n","        {\n","            \"example_id\": i,\n","            \"pred_prob\": model_prediction[\"score\"],\n","            \"pred_label\": model_prediction[\"label\"],\n","            \"true_label\": true_label,\n","        }\n","    )\n","\n","    import matplotlib.pyplot as plt\n","from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n","\n","plt.rcParams[\"font.size\"] = 18  # 文字サイズを大きくする\n","\n","# 混同行列の作成\n","confusion_matrix = confusion_matrix(\n","    y_true=[result[\"true_label\"] for result in results],\n","    y_pred=[result[\"pred_label\"] for result in results],\n","    labels=class_label.names,\n",")\n","# 混同行列を画像として表示\n","ConfusionMatrixDisplay(\n","    confusion_matrix, display_labels=class_label.names\n",").plot()"],"metadata":{"id":"UC-Hl_oAxolg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5.3.3 モデルのショートカットに注意\n"],"metadata":{"id":"ssp5d4Fg1Jza"}},{"cell_type":"code","source":["# 予測が誤った事例を収集\n","failed_results = [\n","    res for res in results if res[\"pred_label\"] != res[\"true_label\"]\n","]\n","# モデルの予測確率が高い順にソート\n","sorted_failed_results = sorted(\n","    failed_results, key=lambda x: -x[\"pred_prob\"]\n",")\n","# 高い確率で予測しながら誤った事例の上位2件を表示\n","for top_result in sorted_failed_results[:5]:\n","    review_text = valid_dataset[top_result[\"example_id\"]][\"sentence\"]\n","    print(f\"レビュー文：{review_text}\")\n","    print(f\"予測：{top_result['pred_label']}\")\n","    print(f\"正解：{top_result['true_label']}\")\n","    print(f\"予測確率: {top_result['pred_prob']:.4f}\")\n","    print(\"----------------\")\n","\n","    text = \"まず，紙ジャケット仕様とありますが，正確には紙ケースです．そのケースはちょうどCD２枚組用のハードケースがぴったり入りそうな紙ケースなのですが，その中にケースより一回り小さな写真集と歌詞カードが入っています．中がすかすかなんですが，私だけでしょうか？これで3780円ですか？？？他のシリーズが出来が良いだけに，なんか残念で☆４つにさせて頂きました\"\n","\n","    print(marc_ja_pipeline(text)[0])\n","\n","    print(marc_ja_pipeline(\"絶対に買わないで。最悪です。\")[0])\n","\n","    print(marc_ja_pipeline(\"絶対に買わないで。最悪です。星４つ。\")[0])"],"metadata":{"id":"xV2Jh36P1Tty"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5.4 自然言語推論・意味的類似度計算・多岐選択式質問応答モデルの実装"],"metadata":{"id":"7xzayS0T3Dik"}},{"cell_type":"markdown","source":["5.4.1 自然言語推論"],"metadata":{"id":"0J-m9-Nj3cK5"}},{"cell_type":"markdown","source":["・ダウンロードコードによる\n"],"metadata":{"id":"iqsiNITT3l7S"}},{"cell_type":"code","source":[],"metadata":{"id":"M4r1D_nw3q3b"},"execution_count":null,"outputs":[]}]}