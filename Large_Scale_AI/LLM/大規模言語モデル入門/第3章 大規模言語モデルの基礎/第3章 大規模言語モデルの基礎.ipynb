{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["3.2.4 transformersで使う"],"metadata":{"id":"AsJe4vslLOSy"}},{"cell_type":"code","source":["!pip install transformers[ja,sentencepiece,torch] pandas\n","\n","from transformers import pipeline\n","\n","# 後続するテキストを予測するpipelineを作成\n","generator = pipeline(\n","    \"text-generation\", model=\"abeja/gpt2-large-japanese\"\n",")\n","# \"日本で一番高い山は\"に続くテキストを生成\n","outputs = generator(\"日本で一番高い山は\")\n","print(outputs[0][\"generated_text\"])"],"metadata":{"id":"9hhiddFtLlBa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3.3.4 transformersで使う"],"metadata":{"id":"UKXkOPPENW3n"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# マスクされたトークンを予測するpipelineを作成\n","fill_mask = pipeline(\n","    \"fill-mask\", model=\"cl-tohoku/bert-base-japanese-v3\"\n",")\n","masked_text = \"日本の首都は[MASK]である\"\n","# [MASK]部分を予測\n","outputs = fill_mask(masked_text)\n","# 上位3件をテーブルで表示\n","display(pd.DataFrame(outputs[:3]))\n","\n","masked_text = \"今日の映画は刺激的で面白かった。この映画は[MASK]。\"\n","outputs = fill_mask(masked_text)\n","display(pd.DataFrame(outputs[:3]))"],"metadata":{"id":"TfH8PIyoNlUb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3.4.4 transformersで使う"],"metadata":{"id":"f_YTMM-UOYG3"}},{"cell_type":"code","source":["# text-to-textで生成するpipelineを作成\n","t2t_generator = pipeline(\n","    \"text2text-generation\", model=\"retrieva-jp/t5-large-long\"\n",")\n","# マスクされたスパンを予測\n","masked_text = \"江戸幕府を開いたのは、<extra_id_0>である\"\n","outputs = t2t_generator(masked_text, eos_token_id=32098)\n","print(outputs[0][\"generated_text\"])\n","\n","t2t_generator.tokenizer.convert_tokens_to_ids(\"<extra_id_1>\")\n","\n","masked_text = \"日本で通貨を発行しているのは、<extra_id_0>である\"\n","outputs = t2t_generator(masked_text, eos_token_id=32098)\n","print(outputs[0][\"generated_text\"])\n","\n","\"日本銀行\" in t2t_generator.tokenizer.vocab"],"metadata":{"id":"mKDZ5QYQOfkQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3.6.1 バイト対符号"],"metadata":{"id":"ELjD4L6ePdZj"}},{"cell_type":"code","source":["# 単語とその頻度\n","word_freqs = {\n","    \"たのしい\": 6,\n","    \"たのしさ\": 2,\n","    \"うつくしい\": 4,\n","    \"うつくしさ\": 1,\n","}\n","# 語彙を文字で初期化\n","vocab = sorted(set([char for word in word_freqs for char in word]))\n","# 単語とその分割状態\n","splits = {word: [char for char in word] for word in word_freqs}\n","\n","from collections import Counter\n","\n","def compute_most_frequent_pair(\n","    splits: dict[str, list[str]]\n",") -> tuple[str, str]:\n","    \"\"\"\n","    最も頻度の高い隣接するサブワードの組を計算する\n","    \"\"\"\n","    pair_freqs = Counter()  # サブワードの組のカウンタ\n","    for word, freq in word_freqs.items():  # すべての単語を処理\n","        split = splits[word]  # 現在の単語の分割状態を取得\n","        # すべての隣接したサブワードの組を処理\n","        for i in range(len(split) - 1):\n","            pair = (split[i], split[i + 1])\n","            # サブワードの組の頻度に単語の頻度を加算\n","            pair_freqs[pair] += freq\n","    # カウンタから最も頻度の高いサブワードの組を取得\n","    pair, _ = pair_freqs.most_common(1)[0]\n","    return pair\n","\n","def merge_pair(\n","    target_pair: tuple[str, str], splits: dict[str, list[str]]\n",") -> dict[str, list[str]]:\n","    \"\"\"\n","    サブワードの組を結合する\n","    \"\"\"\n","    l_str, r_str = target_pair\n","    for word in word_freqs:  # すべての単語を処理\n","        split = splits[word]  # 現在の単語の分割状態を取得\n","        i = 0\n","        # すべての隣接したサブワードの組を処理\n","        while i < len(split) - 1:\n","            # サブワードの組が結合対象と一致したら結合\n","            if split[i] == l_str and split[i + 1] == r_str:\n","                split = split[:i] + [l_str + r_str] + split[i + 2 :]\n","            i += 1\n","        splits[word] = split  # 現在の結合状態を更新\n","    return splits\n","\n","    for step in range(9):\n","    # 最も頻度の高い隣接するサブワードの組を計算\n","    target_pair = compute_most_frequent_pair(splits)\n","    # サブワードの組を結合\n","    splits = merge_pair(target_pair, splits)\n","    # 語彙にサブワードの組を追加\n","    vocab.append(target_pair)\n","\n","    print(vocab)"],"metadata":{"id":"jyN-EBK4Pskp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3.6.3 日本語の扱い"],"metadata":{"id":"Es0ZhAYlQvQY"}},{"cell_type":"code","source":["!pip install transformers[ja,sentencepiece,torch]\n","\n","from transformers import AutoTokenizer\n","\n","mbert_tokenizer = AutoTokenizer.from_pretrained(\n","    \"bert-base-multilingual-cased\"\n",")\n","print(mbert_tokenizer.tokenize(\"自然言語処理\"))\n","\n","print(mbert_tokenizer.tokenize(\"自然言語処理にディープラーニングを使う\"))\n","\n","xlmr_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n","print(xlmr_tokenizer.tokenize(\"自然言語処理にディープラーニングを使う\"))\n","\n","print(xlmr_tokenizer.tokenize(\"私は日本で生まれました\"))\n","\n","print(xlmr_tokenizer.tokenize(\"本日はよろしくお願いいたします\"))\n","\n","bert_ja_tokenizer = AutoTokenizer.from_pretrained(\n","    \"cl-tohoku/bert-base-japanese-v3\"\n",")\n","print(\n","    bert_ja_tokenizer.tokenize(\"自然言語処理にディープラーニングを使う\")\n",")\n","\n","print(bert_ja_tokenizer.tokenize(\"私は日本で生まれました\"))\n"],"metadata":{"id":"jwRzFJEqRG3o"},"execution_count":null,"outputs":[]}]}