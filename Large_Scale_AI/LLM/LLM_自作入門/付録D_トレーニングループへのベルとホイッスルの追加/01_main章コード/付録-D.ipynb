{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPjjegmaa0TjEgMJE3lNS89"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Appendix D: Adding Bells and Whistles to the Training Loop"],"metadata":{"id":"EuCV-1yej3O4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zChhURwlFakS"},"outputs":[],"source":["from importlib.metadata import version\n","import torch\n","\n","print(\"torch version:\", version(\"torch\"))\n","\n","\n","from previous_chapters import GPTModel\n","# If the `previous_chapters.py` file is not available locally,\n","# you can import it from the `llms-from-scratch` PyPI package.\n","# For details, see: https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n","# E.g.,\n","# from llms_from_scratch.ch04 import GPTModel\n","\n","GPT_CONFIG_124M = {\n","    \"vocab_size\": 50257,   # Vocabulary size\n","    \"context_length\": 256, # Shortened context length (orig: 1024)\n","    \"emb_dim\": 768,        # Embedding dimension\n","    \"n_heads\": 12,         # Number of attention heads\n","    \"n_layers\": 12,        # Number of layers\n","    \"drop_rate\": 0.1,      # Dropout rate\n","    \"qkv_bias\": False      # Query-key-value bias\n","}\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Note:\n","# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n","# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n","# However, the resulting loss values may be slightly different.\n","\n","#if torch.cuda.is_available():\n","#    device = torch.device(\"cuda\")\n","#elif torch.backends.mps.is_available():\n","#    device = torch.device(\"mps\")\n","#else:\n","#    device = torch.device(\"cpu\")\n","#\n","# print(f\"Using {device} device.\")\n","\n","torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","model.eval();  # Disable dropout during inference"]},{"cell_type":"code","source":["import os\n","import urllib.request\n","\n","file_path = \"the-verdict.txt\"\n","url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n","\n","if not os.path.exists(file_path):\n","    with urllib.request.urlopen(url) as response:\n","        text_data = response.read().decode('utf-8')\n","    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n","        file.write(text_data)\n","else:\n","    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","        text_data = file.read()"],"metadata":{"id":"khus7kZXj82r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from previous_chapters import create_dataloader_v1\n","# Alternatively:\n","# from llms_from_scratch.ch02 import create_dataloader_v1\n","\n","\n","# Train/validation ratio\n","train_ratio = 0.90\n","split_idx = int(train_ratio * len(text_data))\n","\n","\n","torch.manual_seed(123)\n","\n","train_loader = create_dataloader_v1(\n","    text_data[:split_idx],\n","    batch_size=2,\n","    max_length=GPT_CONFIG_124M[\"context_length\"],\n","    stride=GPT_CONFIG_124M[\"context_length\"],\n","    drop_last=True,\n","    shuffle=True,\n","    num_workers=0\n",")\n","\n","val_loader = create_dataloader_v1(\n","    text_data[split_idx:],\n","    batch_size=2,\n","    max_length=GPT_CONFIG_124M[\"context_length\"],\n","    stride=GPT_CONFIG_124M[\"context_length\"],\n","    drop_last=False,\n","    shuffle=False,\n","    num_workers=0\n",")"],"metadata":{"id":"xMeDDAB2j9UN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["D.1 Learning rate warmup"],"metadata":{"id":"BFqdeNKm3okv"}},{"cell_type":"code","source":["n_epochs = 15\n","initial_lr = 0.0001\n","peak_lr = 0.01"],"metadata":{"id":"biX1oJm-j9uc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_steps = len(train_loader) * n_epochs\n","warmup_steps = int(0.2 * total_steps) # 20% warmup\n","print(warmup_steps)"],"metadata":{"id":"J0in2tg933d_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr_increment = (peak_lr - initial_lr) / warmup_steps\n","\n","global_step = -1\n","track_lrs = []\n","\n","optimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.1)\n","\n","for epoch in range(n_epochs):\n","    for input_batch, target_batch in train_loader:\n","        optimizer.zero_grad()\n","        global_step += 1\n","\n","        if global_step < warmup_steps:\n","            lr = initial_lr + global_step * lr_increment\n","        else:\n","            lr = peak_lr\n","\n","        # Apply the calculated learning rate to the optimizer\n","        for param_group in optimizer.param_groups:\n","            param_group[\"lr\"] = lr\n","        track_lrs.append(optimizer.param_groups[0][\"lr\"])\n","\n","        # Calculate loss and update weights\n","        # ..."],"metadata":{"id":"SOnUmU7p34Nl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(5, 3))\n","plt.ylabel(\"Learning rate\")\n","plt.xlabel(\"Step\")\n","total_training_steps = len(train_loader) * n_epochs\n","plt.plot(range(total_training_steps), track_lrs)\n","plt.tight_layout(); plt.savefig(\"1.pdf\")\n","plt.show()"],"metadata":{"id":"6D7oE1Cp34k_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["D.2 Cosine decay"],"metadata":{"id":"_kbuX_kX4lcP"}},{"cell_type":"code","source":["import math\n","\n","min_lr = 0.1 * initial_lr\n","track_lrs = []\n","\n","lr_increment = (peak_lr - initial_lr) / warmup_steps\n","global_step = -1\n","\n","for epoch in range(n_epochs):\n","    for input_batch, target_batch in train_loader:\n","        optimizer.zero_grad()\n","        global_step += 1\n","\n","        # Adjust the learning rate based on the current phase (warmup or cosine annealing)\n","        if global_step < warmup_steps:\n","            # Linear warmup\n","            lr = initial_lr + global_step * lr_increment\n","        else:\n","            # Cosine annealing after warmup\n","            progress = ((global_step - warmup_steps) /\n","                        (total_training_steps - warmup_steps))\n","            lr = min_lr + (peak_lr - min_lr) * 0.5 * (\n","                1 + math.cos(math.pi * progress))\n","\n","        # Apply the calculated learning rate to the optimizer\n","        for param_group in optimizer.param_groups:\n","            param_group[\"lr\"] = lr\n","        track_lrs.append(optimizer.param_groups[0][\"lr\"])\n","\n","        # Calculate loss and update weights"],"metadata":{"id":"QGtZwb17349n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(5, 3))\n","plt.ylabel(\"Learning rate\")\n","plt.xlabel(\"Step\")\n","plt.plot(range(total_training_steps), track_lrs)\n","plt.tight_layout(); plt.savefig(\"2.pdf\")\n","plt.show()"],"metadata":{"id":"ef_yGzbg4Z3Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["D.3 Gradient clipping"],"metadata":{"id":"mNdcERjv5N5k"}},{"cell_type":"code","source":["from previous_chapters import calc_loss_batch\n","# Alternatively:\n","# from llms_from_scratch.ch05 import calc_loss_batch\n","\n","\n","torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","model.to(device)\n","\n","loss = calc_loss_batch(input_batch, target_batch, model, device)\n","loss.backward()"],"metadata":{"id":"vLAmqJAd4aPg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def find_highest_gradient(model):\n","    max_grad = None\n","    for param in model.parameters():\n","        if param.grad is not None:\n","            grad_values = param.grad.data.flatten()\n","            max_grad_param = grad_values.max()\n","            if max_grad is None or max_grad_param > max_grad:\n","                max_grad = max_grad_param\n","    return max_grad\n","\n","print(find_highest_gradient(model))"],"metadata":{"id":"jTIgeMFF4akt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["D.4 The modified training function"],"metadata":{"id":"3DlVGlTd5wUF"}},{"cell_type":"code","source":["from previous_chapters import evaluate_model, generate_and_print_sample\n","# Alternatively:\n","# from llms_from_scratch.ch05 import evaluate_model, generate_and_print_samplee\n","\n","\n","ORIG_BOOK_VERSION = False\n","\n","\n","def train_model(model, train_loader, val_loader, optimizer, device,\n","                n_epochs, eval_freq, eval_iter, start_context, tokenizer,\n","                warmup_steps, initial_lr=3e-05, min_lr=1e-6):\n","\n","    train_losses, val_losses, track_tokens_seen, track_lrs = [], [], [], []\n","    tokens_seen, global_step = 0, -1\n","\n","    # Retrieve the maximum learning rate from the optimizer\n","    peak_lr = optimizer.param_groups[0][\"lr\"]\n","\n","    # Calculate the total number of iterations in the training process\n","    total_training_steps = len(train_loader) * n_epochs\n","\n","    # Calculate the learning rate increment during the warmup phase\n","    lr_increment = (peak_lr - initial_lr) / warmup_steps\n","\n","    for epoch in range(n_epochs):\n","        model.train()\n","        for input_batch, target_batch in train_loader:\n","            optimizer.zero_grad()\n","            global_step += 1\n","\n","            # Adjust the learning rate based on the current phase (warmup or cosine annealing)\n","            if global_step < warmup_steps:\n","                # Linear warmup\n","                lr = initial_lr + global_step * lr_increment\n","            else:\n","                # Cosine annealing after warmup\n","                progress = ((global_step - warmup_steps) /\n","                            (total_training_steps - warmup_steps))\n","                lr = min_lr + (peak_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * progress))\n","\n","            # Apply the calculated learning rate to the optimizer\n","            for param_group in optimizer.param_groups:\n","                param_group[\"lr\"] = lr\n","            track_lrs.append(lr)  # Store the current learning rate\n","\n","            # Calculate and backpropagate the loss\n","            loss = calc_loss_batch(input_batch, target_batch, model, device)\n","            loss.backward()\n","\n","            # Apply gradient clipping after the warmup phase to avoid exploding gradients\n","            if ORIG_BOOK_VERSION:\n","                if global_step > warmup_steps:\n","                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","            else:\n","                if global_step >= warmup_steps:  # the book originally used global_step > warmup_steps, which lead to a skipped clipping step after warmup\n","                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","            optimizer.step()\n","            tokens_seen += input_batch.numel()\n","\n","            # Periodically evaluate the model on the training and validation sets\n","            if global_step % eval_freq == 0:\n","                train_loss, val_loss = evaluate_model(\n","                    model, train_loader, val_loader,\n","                    device, eval_iter\n","                )\n","                train_losses.append(train_loss)\n","                val_losses.append(val_loss)\n","                track_tokens_seen.append(tokens_seen)\n","                # Print the current losses\n","                print(f\"Ep {epoch+1} (Iter {global_step:06d}): \"\n","                      f\"Train loss {train_loss:.3f}, \"\n","                      f\"Val loss {val_loss:.3f}\"\n","                )\n","\n","        # Generate and print a sample from the model to monitor progress\n","        generate_and_print_sample(\n","            model, tokenizer, device, start_context\n","        )\n","\n","    return train_losses, val_losses, track_tokens_seen, track_lrs"],"metadata":{"id":"uMbiCqu5584P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tiktoken\n","\n","# Note:\n","# Uncomment the following code to calculate the execution time\n","# import time\n","# start_time = time.time()\n","\n","torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","model.to(device)\n","\n","peak_lr = 0.001  # this was originally set to 5e-4 in the book by mistake\n","optimizer = torch.optim.AdamW(model.parameters(), lr=peak_lr, weight_decay=0.1)  # the book accidentally omitted the lr assignment\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","n_epochs = 15\n","train_losses, val_losses, tokens_seen, lrs = train_model(\n","    model, train_loader, val_loader, optimizer, device, n_epochs=n_epochs,\n","    eval_freq=5, eval_iter=1, start_context=\"Every effort moves you\",\n","    tokenizer=tokenizer, warmup_steps=warmup_steps,\n","    initial_lr=1e-5, min_lr=1e-5\n",")\n","\n","# Note:\n","# Uncomment the following code to show the execution time\n","# end_time = time.time()\n","# execution_time_minutes = (end_time - start_time) / 60\n","# print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"],"metadata":{"id":"nLSPK_Im5-TN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(5, 3))\n","plt.plot(range(len(lrs)), lrs)\n","plt.ylabel(\"Learning rate\")\n","plt.xlabel(\"Steps\")\n","plt.show()"],"metadata":{"id":"EBY4dScI59VH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from previous_chapters import plot_losses\n","# Alternatively:\n","# from llms_from_scratch.ch05 import plot_losses\n","\n","\n","epochs_tensor = torch.linspace(1, n_epochs, len(train_losses))\n","plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n","plt.tight_layout(); plt.savefig(\"3.pdf\")\n","plt.show()"],"metadata":{"id":"YmWNDx0T6o4d"},"execution_count":null,"outputs":[]}]}