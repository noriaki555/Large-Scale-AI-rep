{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNKigGegDP/IJ4TnwYCNc1W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Chapter 7: Finetuning To Follow Instructions"],"metadata":{"id":"DNq0CjAGWEFB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LmL_uvJ2hQRg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749762522423,"user_tz":-540,"elapsed":53,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"f6cdac6f-f755-42f7-b744-110418c94076"},"outputs":[{"output_type":"stream","name":"stdout","text":["numpy version: 2.0.2\n","matplotlib version: 3.10.0\n","tiktoken version: 0.9.0\n","torch version: 2.6.0+cu124\n","tqdm version: 4.67.1\n","tensorflow version: 2.18.0\n"]}],"source":["from importlib.metadata import version\n","\n","pkgs = [\n","    \"numpy\",       # PyTorch & TensorFlow dependency\n","    \"matplotlib\",  # Plotting library\n","    \"tiktoken\",    # Tokenizer\n","    \"torch\",       # Deep learning library\n","    \"tqdm\",        # Progress bar\n","    \"tensorflow\",  # For OpenAI's pretrained weights\n","]\n","for p in pkgs:\n","    print(f\"{p} version: {version(p)}\")"]},{"cell_type":"markdown","source":["7.2 Preparing a dataset for supervised instruction finetuning"],"metadata":{"id":"uq6e0vMVWVAE"}},{"cell_type":"code","source":["import json\n","import os\n","import urllib\n","\n","\n","def download_and_load_file(file_path, url):\n","\n","    if not os.path.exists(file_path):\n","        with urllib.request.urlopen(url) as response:\n","            text_data = response.read().decode(\"utf-8\")\n","        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n","            file.write(text_data)\n","\n","    # The book originally contained this unnecessary \"else\" clause:\n","    #else:\n","    #    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","    #        text_data = file.read()\n","\n","    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","        data = json.load(file)\n","\n","    return data\n","\n","\n","file_path = \"instruction-data.json\"\n","url = (\n","    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n","    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",")\n","\n","data = download_and_load_file(file_path, url)\n","print(\"Number of entries:\", len(data))"],"metadata":{"id":"tk-AglkcVz4m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749764992805,"user_tz":-540,"elapsed":190,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"f19c4446-0df8-4754-d877-55d40c4e335b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of entries: 1100\n"]}]},{"cell_type":"code","source":["print(\"Example entry:\\n\", data[50])"],"metadata":{"id":"_nOXXvRTWnY6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749764530347,"user_tz":-540,"elapsed":52,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"84a392fa-0bf5-441c-ee74-71e981b09f82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Example entry:\n"," {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"]}]},{"cell_type":"code","source":["print(\"Another example entry:\\n\", data[999])"],"metadata":{"id":"mC3QNnnDW309","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749764537340,"user_tz":-540,"elapsed":51,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"0bc41d06-8903-4895-94cb-6095ed26088a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Another example entry:\n"," {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n"]}]},{"cell_type":"code","source":["def format_input(entry):\n","    instruction_text = (\n","        f\"Below is an instruction that describes a task. \"\n","        f\"Write a response that appropriately completes the request.\"\n","        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n","    )\n","\n","    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n","\n","    return instruction_text + input_text"],"metadata":{"id":"CN5iX9hLXAq6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_input = format_input(data[50])\n","desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n","\n","print(model_input + desired_response)"],"metadata":{"id":"4s13fOzsXJtD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749764542410,"user_tz":-540,"elapsed":20,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"58f6f541-1efc-4473-b340-06606313b678"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","Identify the correct spelling of the following word.\n","\n","### Input:\n","Ocassion\n","\n","### Response:\n","The correct spelling is 'Occasion.'\n"]}]},{"cell_type":"code","source":["train_portion = int(len(data) * 0.85)  # 85% for training\n","test_portion = int(len(data) * 0.1)    # 10% for testing\n","val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n","\n","train_data = data[:train_portion]\n","test_data = data[train_portion:train_portion + test_portion]\n","val_data = data[train_portion + test_portion:]"],"metadata":{"id":"1oWvVvITX9fy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Training set length:\", len(train_data))\n","print(\"Validation set length:\", len(val_data))\n","print(\"Test set length:\", len(test_data))"],"metadata":{"id":"MxOPYCAWYFkh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749762542090,"user_tz":-540,"elapsed":12,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"cccfb7fe-6fcd-4676-f29c-cdb837cb4140"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training set length: 935\n","Validation set length: 55\n","Test set length: 110\n"]}]},{"cell_type":"markdown","source":["7.3 Organizing data into training batches"],"metadata":{"id":"SM0P_64DYPBb"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","\n","\n","class InstructionDataset(Dataset):\n","    def __init__(self, data, tokenizer):\n","        self.data = data\n","\n","        # Pre-tokenize texts\n","        self.encoded_texts = []\n","        for entry in data:\n","            instruction_plus_input = format_input(entry)\n","            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n","            full_text = instruction_plus_input + response_text\n","            self.encoded_texts.append(\n","                tokenizer.encode(full_text)\n","            )\n","\n","    def __getitem__(self, index):\n","        return self.encoded_texts[index]\n","\n","    def __len__(self):\n","        return len(self.data)"],"metadata":{"id":"0L_fnPvQYZEC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tiktoken\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"],"metadata":{"id":"Y51roNaoYpti","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749765124998,"user_tz":-540,"elapsed":841,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"ff363a7e-3c26-43b8-dbf7-1db05cf32eb7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[50256]\n"]}]},{"cell_type":"code","source":["def custom_collate_draft_1(\n","    batch,\n","    pad_token_id=50256,\n","    device=\"cpu\"\n","):\n","    # Find the longest sequence in the batch\n","    # and increase the max length by +1, which will add one extra\n","    # padding token below\n","    batch_max_length = max(len(item)+1 for item in batch)\n","\n","    # Pad and prepare inputs\n","    inputs_lst = []\n","\n","    for item in batch:\n","        new_item = item.copy()\n","        # Add an <|endoftext|> token\n","        new_item += [pad_token_id]\n","        # Pad sequences to batch_max_length\n","        padded = (\n","            new_item + [pad_token_id] *\n","            (batch_max_length - len(new_item))\n","        )\n","        # Via padded[:-1], we remove the extra padded token\n","        # that has been added via the +1 setting in batch_max_length\n","        # (the extra padding token will be relevant in later codes)\n","        inputs = torch.tensor(padded[:-1])\n","        inputs_lst.append(inputs)\n","\n","    # Convert list of inputs to tensor and transfer to target device\n","    inputs_tensor = torch.stack(inputs_lst).to(device)\n","    return inputs_tensor"],"metadata":{"id":"EqDXrV1dYyKq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs_1 = [0, 1, 2, 3, 4]\n","inputs_2 = [5, 6]\n","inputs_3 = [7, 8, 9]\n","\n","batch = (\n","    inputs_1,\n","    inputs_2,\n","    inputs_3\n",")\n","\n","print(custom_collate_draft_1(batch))"],"metadata":{"id":"Hj6T9umnZCcy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749765792287,"user_tz":-540,"elapsed":63,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"53f02359-e835-49af-d9bc-e5b7617d5cec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[    0,     1,     2,     3,     4],\n","        [    5,     6, 50256, 50256, 50256],\n","        [    7,     8,     9, 50256, 50256]])\n"]}]},{"cell_type":"code","source":["def custom_collate_draft_2(\n","    batch,\n","    pad_token_id=50256,\n","    device=\"cpu\"\n","):\n","    # Find the longest sequence in the batch\n","    batch_max_length = max(len(item)+1 for item in batch)\n","\n","    # Pad and prepare inputs\n","    inputs_lst, targets_lst = [], []\n","\n","    for item in batch:\n","        new_item = item.copy()\n","        # Add an <|endoftext|> token\n","        new_item += [pad_token_id]\n","        # Pad sequences to max_length\n","        padded = (\n","            new_item + [pad_token_id] *\n","            (batch_max_length - len(new_item))\n","        )\n","        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n","        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n","        inputs_lst.append(inputs)\n","        targets_lst.append(targets)\n","\n","    # Convert list of inputs to tensor and transfer to target device\n","    inputs_tensor = torch.stack(inputs_lst).to(device)\n","    targets_tensor = torch.stack(targets_lst).to(device)\n","    return inputs_tensor, targets_tensor"],"metadata":{"id":"VXKvBC9bZK5p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs, targets = custom_collate_draft_2(batch)\n","print(inputs)\n","print(targets)"],"metadata":{"id":"mSpYfkGNZcVm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749764434768,"user_tz":-540,"elapsed":36,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"956ad299-8d4f-4af1-b95b-976402b23a9b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[    0,     1,     2,     3,     4],\n","        [    5,     6, 50256, 50256, 50256],\n","        [    7,     8,     9, 50256, 50256]])\n","tensor([[    1,     2,     3,     4, 50256],\n","        [    6, 50256, 50256, 50256, 50256],\n","        [    8,     9, 50256, 50256, 50256]])\n"]}]},{"cell_type":"code","source":["def custom_collate_fn(\n","    batch,\n","    pad_token_id=50256,\n","    ignore_index=-100,\n","    allowed_max_length=None,\n","    device=\"cpu\"\n","):\n","    # Find the longest sequence in the batch\n","    batch_max_length = max(len(item)+1 for item in batch)\n","\n","    # Pad and prepare inputs and targets\n","    inputs_lst, targets_lst = [], []\n","\n","    for item in batch:\n","        new_item = item.copy()\n","        # Add an <|endoftext|> token\n","        new_item += [pad_token_id]\n","        # Pad sequences to max_length\n","        padded = (\n","            new_item + [pad_token_id] *\n","            (batch_max_length - len(new_item))\n","        )\n","        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n","        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n","\n","        # New: Replace all but the first padding tokens in targets by ignore_index\n","        mask = targets == pad_token_id\n","        indices = torch.nonzero(mask).squeeze()\n","        if indices.numel() > 1:\n","            targets[indices[1:]] = ignore_index\n","\n","        # New: Optionally truncate to maximum sequence length\n","        if allowed_max_length is not None:\n","            inputs = inputs[:allowed_max_length]\n","            targets = targets[:allowed_max_length]\n","\n","        inputs_lst.append(inputs)\n","        targets_lst.append(targets)\n","\n","    # Convert list of inputs and targets to tensors and transfer to target device\n","    inputs_tensor = torch.stack(inputs_lst).to(device)\n","    targets_tensor = torch.stack(targets_lst).to(device)\n","\n","    return inputs_tensor, targets_tensor"],"metadata":{"id":"VnMhxSv3ZluB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs, targets = custom_collate_fn(batch)\n","print(inputs)\n","print(targets)"],"metadata":{"id":"RxurC0hkZ1VI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749765836659,"user_tz":-540,"elapsed":20,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"61713be7-1b24-43e4-dc5f-9a21665d6b25"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[    0,     1,     2,     3,     4],\n","        [    5,     6, 50256, 50256, 50256],\n","        [    7,     8,     9, 50256, 50256]])\n","tensor([[    1,     2,     3,     4, 50256],\n","        [    6, 50256,  -100,  -100,  -100],\n","        [    8,     9, 50256,  -100,  -100]])\n"]}]},{"cell_type":"code","source":["logits_1 = torch.tensor(\n","    [[-1.0, 1.0],  # 1st training example\n","     [-0.5, 1.5]]  # 2nd training example\n",")\n","targets_1 = torch.tensor([0, 1])\n","\n","\n","loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n","print(loss_1)"],"metadata":{"id":"qxEdaMVzZ83i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749764443910,"user_tz":-540,"elapsed":56,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"0e6c6427-958b-4689-b51a-397a3389e4b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.1269)\n"]}]},{"cell_type":"code","source":["logits_2 = torch.tensor(\n","    [[-1.0, 1.0],\n","     [-0.5, 1.5],\n","     [-0.5, 1.5]]  # New 3rd training example\n",")\n","targets_2 = torch.tensor([0, 1, 1])\n","\n","loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n","print(loss_2)"],"metadata":{"id":"13dcoFNpaGp5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749764449796,"user_tz":-540,"elapsed":13,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"06f161cd-701e-449b-cfae-bda0e05ab72a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.7936)\n"]}]},{"cell_type":"code","source":["targets_3 = torch.tensor([0, 1, -100])\n","\n","loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n","print(loss_3)\n","print(\"loss_1 == loss_3:\", loss_1 == loss_3)"],"metadata":{"id":"1FQ6kPGVaNqv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749764453027,"user_tz":-540,"elapsed":19,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"6920ab81-27da-41f9-a592-a75c94167df2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.1269)\n","loss_1 == loss_3: tensor(True)\n"]}]},{"cell_type":"code","source":["# Note:\n","# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n","# which is much faster than on an Apple CPU (as measured on an M3 MacBook Air).\n","# However, the resulting loss values may be slightly different.\n","\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","elif torch.backends.mps.is_available():\n","    device = torch.device(\"mps\")\n","else:\n","    device = torch.device(\"cpu\")\n","\n","print(\"Device:\", device)"],"metadata":{"id":"BY161w1PaXmo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749764457915,"user_tz":-540,"elapsed":10,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"061cae2a-81a4-4588-e77d-edbffc990c09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cpu\n"]}]},{"cell_type":"code","source":["from functools import partial\n","\n","customized_collate_fn = partial(\n","    custom_collate_fn,\n","    allowed_max_length=512,\n","    device=device\n",")"],"metadata":{"id":"68AAElLVa45H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","num_workers = 0\n","batch_size = 8\n","\n","torch.manual_seed(123)\n","\n","train_dataset = InstructionDataset(train_data, tokenizer)\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=batch_size,\n","    collate_fn=customized_collate_fn,\n","    shuffle=True,\n","    drop_last=True,\n","    num_workers=num_workers\n",")"],"metadata":{"id":"jW4XL0ZkbA24"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_dataset = InstructionDataset(val_data, tokenizer)\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=batch_size,\n","    collate_fn=customized_collate_fn,\n","    shuffle=False,\n","    drop_last=False,\n","    num_workers=num_workers\n",")\n","\n","test_dataset = InstructionDataset(test_data, tokenizer)\n","test_loader = DataLoader(\n","    test_dataset,\n","    batch_size=batch_size,\n","    collate_fn=customized_collate_fn,\n","    shuffle=False,\n","    drop_last=False,\n","    num_workers=num_workers\n",")"],"metadata":{"id":"cdbcI1B0bIvH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Train loader:\")\n","for inputs, targets in train_loader:\n","    print(inputs.shape, targets.shape)"],"metadata":{"id":"sAFIqFOrbUYP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749628208269,"user_tz":-540,"elapsed":139,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"43fa1f8e-7422-4d14-d877-5b1d1d0a01fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train loader:\n","torch.Size([8, 61]) torch.Size([8, 61])\n","torch.Size([8, 76]) torch.Size([8, 76])\n","torch.Size([8, 73]) torch.Size([8, 73])\n","torch.Size([8, 68]) torch.Size([8, 68])\n","torch.Size([8, 65]) torch.Size([8, 65])\n","torch.Size([8, 72]) torch.Size([8, 72])\n","torch.Size([8, 80]) torch.Size([8, 80])\n","torch.Size([8, 67]) torch.Size([8, 67])\n","torch.Size([8, 62]) torch.Size([8, 62])\n","torch.Size([8, 75]) torch.Size([8, 75])\n","torch.Size([8, 62]) torch.Size([8, 62])\n","torch.Size([8, 68]) torch.Size([8, 68])\n","torch.Size([8, 67]) torch.Size([8, 67])\n","torch.Size([8, 77]) torch.Size([8, 77])\n","torch.Size([8, 69]) torch.Size([8, 69])\n","torch.Size([8, 79]) torch.Size([8, 79])\n","torch.Size([8, 71]) torch.Size([8, 71])\n","torch.Size([8, 66]) torch.Size([8, 66])\n","torch.Size([8, 83]) torch.Size([8, 83])\n","torch.Size([8, 68]) torch.Size([8, 68])\n","torch.Size([8, 80]) torch.Size([8, 80])\n","torch.Size([8, 71]) torch.Size([8, 71])\n","torch.Size([8, 69]) torch.Size([8, 69])\n","torch.Size([8, 65]) torch.Size([8, 65])\n","torch.Size([8, 68]) torch.Size([8, 68])\n","torch.Size([8, 60]) torch.Size([8, 60])\n","torch.Size([8, 59]) torch.Size([8, 59])\n","torch.Size([8, 69]) torch.Size([8, 69])\n","torch.Size([8, 63]) torch.Size([8, 63])\n","torch.Size([8, 65]) torch.Size([8, 65])\n","torch.Size([8, 76]) torch.Size([8, 76])\n","torch.Size([8, 66]) torch.Size([8, 66])\n","torch.Size([8, 71]) torch.Size([8, 71])\n","torch.Size([8, 91]) torch.Size([8, 91])\n","torch.Size([8, 65]) torch.Size([8, 65])\n","torch.Size([8, 64]) torch.Size([8, 64])\n","torch.Size([8, 67]) torch.Size([8, 67])\n","torch.Size([8, 66]) torch.Size([8, 66])\n","torch.Size([8, 64]) torch.Size([8, 64])\n","torch.Size([8, 65]) torch.Size([8, 65])\n","torch.Size([8, 75]) torch.Size([8, 75])\n","torch.Size([8, 89]) torch.Size([8, 89])\n","torch.Size([8, 59]) torch.Size([8, 59])\n","torch.Size([8, 88]) torch.Size([8, 88])\n","torch.Size([8, 83]) torch.Size([8, 83])\n","torch.Size([8, 83]) torch.Size([8, 83])\n","torch.Size([8, 70]) torch.Size([8, 70])\n","torch.Size([8, 65]) torch.Size([8, 65])\n","torch.Size([8, 74]) torch.Size([8, 74])\n","torch.Size([8, 76]) torch.Size([8, 76])\n","torch.Size([8, 67]) torch.Size([8, 67])\n","torch.Size([8, 75]) torch.Size([8, 75])\n","torch.Size([8, 83]) torch.Size([8, 83])\n","torch.Size([8, 69]) torch.Size([8, 69])\n","torch.Size([8, 67]) torch.Size([8, 67])\n","torch.Size([8, 60]) torch.Size([8, 60])\n","torch.Size([8, 60]) torch.Size([8, 60])\n","torch.Size([8, 66]) torch.Size([8, 66])\n","torch.Size([8, 80]) torch.Size([8, 80])\n","torch.Size([8, 71]) torch.Size([8, 71])\n","torch.Size([8, 61]) torch.Size([8, 61])\n","torch.Size([8, 58]) torch.Size([8, 58])\n","torch.Size([8, 71]) torch.Size([8, 71])\n","torch.Size([8, 67]) torch.Size([8, 67])\n","torch.Size([8, 68]) torch.Size([8, 68])\n","torch.Size([8, 63]) torch.Size([8, 63])\n","torch.Size([8, 87]) torch.Size([8, 87])\n","torch.Size([8, 68]) torch.Size([8, 68])\n","torch.Size([8, 64]) torch.Size([8, 64])\n","torch.Size([8, 68]) torch.Size([8, 68])\n","torch.Size([8, 71]) torch.Size([8, 71])\n","torch.Size([8, 68]) torch.Size([8, 68])\n","torch.Size([8, 71]) torch.Size([8, 71])\n","torch.Size([8, 61]) torch.Size([8, 61])\n","torch.Size([8, 65]) torch.Size([8, 65])\n","torch.Size([8, 67]) torch.Size([8, 67])\n","torch.Size([8, 65]) torch.Size([8, 65])\n","torch.Size([8, 64]) torch.Size([8, 64])\n","torch.Size([8, 60]) torch.Size([8, 60])\n","torch.Size([8, 72]) torch.Size([8, 72])\n","torch.Size([8, 64]) torch.Size([8, 64])\n","torch.Size([8, 70]) torch.Size([8, 70])\n","torch.Size([8, 57]) torch.Size([8, 57])\n","torch.Size([8, 72]) torch.Size([8, 72])\n","torch.Size([8, 64]) torch.Size([8, 64])\n","torch.Size([8, 68]) torch.Size([8, 68])\n","torch.Size([8, 62]) torch.Size([8, 62])\n","torch.Size([8, 74]) torch.Size([8, 74])\n","torch.Size([8, 80]) torch.Size([8, 80])\n","torch.Size([8, 68]) torch.Size([8, 68])\n","torch.Size([8, 70]) torch.Size([8, 70])\n","torch.Size([8, 91]) torch.Size([8, 91])\n","torch.Size([8, 61]) torch.Size([8, 61])\n","torch.Size([8, 66]) torch.Size([8, 66])\n","torch.Size([8, 80]) torch.Size([8, 80])\n","torch.Size([8, 81]) torch.Size([8, 81])\n","torch.Size([8, 74]) torch.Size([8, 74])\n","torch.Size([8, 82]) torch.Size([8, 82])\n","torch.Size([8, 63]) torch.Size([8, 63])\n","torch.Size([8, 83]) torch.Size([8, 83])\n","torch.Size([8, 68]) torch.Size([8, 68])\n","torch.Size([8, 67]) torch.Size([8, 67])\n","torch.Size([8, 77]) torch.Size([8, 77])\n","torch.Size([8, 91]) torch.Size([8, 91])\n","torch.Size([8, 64]) torch.Size([8, 64])\n","torch.Size([8, 61]) torch.Size([8, 61])\n","torch.Size([8, 75]) torch.Size([8, 75])\n","torch.Size([8, 64]) torch.Size([8, 64])\n","torch.Size([8, 66]) torch.Size([8, 66])\n","torch.Size([8, 78]) torch.Size([8, 78])\n","torch.Size([8, 66]) torch.Size([8, 66])\n","torch.Size([8, 64]) torch.Size([8, 64])\n","torch.Size([8, 83]) torch.Size([8, 83])\n","torch.Size([8, 66]) torch.Size([8, 66])\n","torch.Size([8, 74]) torch.Size([8, 74])\n","torch.Size([8, 69]) torch.Size([8, 69])\n"]}]},{"cell_type":"code","source":["print(inputs[0])"],"metadata":{"id":"AMsk3VlLbgZ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(targets[0])"],"metadata":{"id":"t9hfplL5bpIw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["7.5 Loading a pretrained LLM"],"metadata":{"id":"a6-gcnKab76V"}},{"cell_type":"code","source":["#from gpt_download import download_and_load_gpt2\n","#from previous_chapters import GPTModel, load_weights_into_gpt\n","# If the `previous_chapters.py` file is not available locally,\n","# you can import it from the `llms-from-scratch` PyPI package.\n","# For details, see: https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n","# E.g.,\n","# from llms_from_scratch.ch04 import GPTModel\n","# from llms_from_scratch.ch05 import download_and_load_gpt2, load_weights_into_gpt\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import os\n","import urllib\n","import json\n","from tqdm import tqdm\n","import tensorflow as tf\n","\n","def download_and_load_gpt2(model_size, models_dir):\n","    # Validate model size\n","    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n","    if model_size not in allowed_sizes:\n","        raise ValueError(f\"Model size not in {allowed_sizes}\")\n","\n","    # Define paths\n","    model_dir = os.path.join(models_dir, model_size)\n","    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n","    backup_base_url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2\"\n","    filenames = [\n","        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n","        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n","        \"model.ckpt.meta\", \"vocab.bpe\"\n","    ]\n","\n","    # Download files\n","    os.makedirs(model_dir, exist_ok=True)\n","    for filename in filenames:\n","        file_url = os.path.join(base_url, model_size, filename)\n","        backup_url = os.path.join(backup_base_url, model_size, filename)\n","        file_path = os.path.join(model_dir, filename)\n","        download_file(file_url, file_path, backup_url)\n","\n","    # Load settings and params\n","    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n","    settings = json.load(open(os.path.join(model_dir, \"hparams.json\"), \"r\", encoding=\"utf-8\"))\n","    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n","\n","    return settings, params\n","\n","\n","def download_file(url, destination, backup_url=None):\n","    def _attempt_download(download_url):\n","        with urllib.request.urlopen(download_url) as response:\n","            # Get the total file size from headers, defaulting to 0 if not present\n","            file_size = int(response.headers.get(\"Content-Length\", 0))\n","\n","            # Check if file exists and has the same size\n","            if os.path.exists(destination):\n","                file_size_local = os.path.getsize(destination)\n","                if file_size == file_size_local:\n","                    print(f\"File already exists and is up-to-date: {destination}\")\n","                    return True  # Indicate success without re-downloading\n","\n","            block_size = 1024  # 1 Kilobyte\n","\n","            # Initialize the progress bar with total file size\n","            progress_bar_description = os.path.basename(download_url)\n","            with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n","                with open(destination, \"wb\") as file:\n","                    while True:\n","                        chunk = response.read(block_size)\n","                        if not chunk:\n","                            break\n","                        file.write(chunk)\n","                        progress_bar.update(len(chunk))\n","            return True\n","\n","    try:\n","        if _attempt_download(url):\n","            return\n","    except (urllib.error.HTTPError, urllib.error.URLError):\n","        if backup_url is not None:\n","            print(f\"Primary URL ({url}) failed. Attempting backup URL: {backup_url}\")\n","            try:\n","                if _attempt_download(backup_url):\n","                    return\n","            except urllib.error.HTTPError:\n","                pass\n","\n","        # If we reach here, both attempts have failed\n","        error_message = (\n","            f\"Failed to download from both primary URL ({url})\"\n","            f\"{' and backup URL (' + backup_url + ')' if backup_url else ''}.\"\n","            \"\\nCheck your internet connection or the file availability.\\n\"\n","            \"For help, visit: https://github.com/rasbt/LLMs-from-scratch/discussions/273\"\n","        )\n","        print(error_message)\n","    except Exception as e:\n","        print(f\"An unexpected error occurred: {e}\")\n","\n","\n","# Alternative way using `requests`\n","\"\"\"\n","def download_file(url, destination):\n","    # Send a GET request to download the file in streaming mode\n","    response = requests.get(url, stream=True)\n","\n","    # Get the total file size from headers, defaulting to 0 if not present\n","    file_size = int(response.headers.get(\"content-length\", 0))\n","\n","    # Check if file exists and has the same size\n","    if os.path.exists(destination):\n","        file_size_local = os.path.getsize(destination)\n","        if file_size == file_size_local:\n","            print(f\"File already exists and is up-to-date: {destination}\")\n","            return\n","\n","    # Define the block size for reading the file\n","    block_size = 1024  # 1 Kilobyte\n","\n","    # Initialize the progress bar with total file size\n","    progress_bar_description = url.split(\"/\")[-1]  # Extract filename from URL\n","    with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n","        # Open the destination file in binary write mode\n","        with open(destination, \"wb\") as file:\n","            # Iterate over the file data in chunks\n","            for chunk in response.iter_content(block_size):\n","                progress_bar.update(len(chunk))  # Update progress bar\n","                file.write(chunk)  # Write the chunk to the file\n","\"\"\"\n","\n","def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n","    # Initialize parameters dictionary with empty blocks for each layer\n","    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n","\n","    # Iterate over each variable in the checkpoint\n","    for name, _ in tf.train.list_variables(ckpt_path):\n","        # Load the variable and remove singleton dimensions\n","        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n","\n","        # Process the variable name to extract relevant parts\n","        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n","\n","        # Identify the target dictionary for the variable\n","        target_dict = params\n","        if variable_name_parts[0].startswith(\"h\"):\n","            layer_number = int(variable_name_parts[0][1:])\n","            target_dict = params[\"blocks\"][layer_number]\n","\n","        # Recursively access or create nested dictionaries\n","        for key in variable_name_parts[1:-1]:\n","            target_dict = target_dict.setdefault(key, {})\n","\n","        # Assign the variable array to the last key\n","        last_key = variable_name_parts[-1]\n","        target_dict[last_key] = variable_array\n","\n","    return params\n","\n","def assign(left,right):\n","  if left.shape != right.shape:\n","    raise ValueError(f\"Shape mismatch.Left:{left.shape},\"\n","    f\"Right:{right.shape}\")\n","  return torch.nn.Parameter(torch.tensor(right))\n","\n","def load_weights_into_gpt(gpt, params):\n","    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n","    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n","\n","    for b in range(len(params[\"blocks\"])):\n","        q_w, k_w, v_w = np.split(\n","            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n","        gpt.trf_blocks[b].att.W_query.weight = assign(\n","            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n","        gpt.trf_blocks[b].att.W_key.weight = assign(\n","            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n","        gpt.trf_blocks[b].att.W_value.weight = assign(\n","            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n","\n","        q_b, k_b, v_b = np.split(\n","            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n","        gpt.trf_blocks[b].att.W_query.bias = assign(\n","            gpt.trf_blocks[b].att.W_query.bias, q_b)\n","        gpt.trf_blocks[b].att.W_key.bias = assign(\n","            gpt.trf_blocks[b].att.W_key.bias, k_b)\n","        gpt.trf_blocks[b].att.W_value.bias = assign(\n","            gpt.trf_blocks[b].att.W_value.bias, v_b)\n","\n","        gpt.trf_blocks[b].att.out_proj.weight = assign(\n","            gpt.trf_blocks[b].att.out_proj.weight,\n","            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n","        gpt.trf_blocks[b].att.out_proj.bias = assign(\n","            gpt.trf_blocks[b].att.out_proj.bias,\n","            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n","\n","        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n","            gpt.trf_blocks[b].ff.layers[0].weight,\n","            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n","        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n","            gpt.trf_blocks[b].ff.layers[0].bias,\n","            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n","        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n","            gpt.trf_blocks[b].ff.layers[2].weight,\n","            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n","        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n","            gpt.trf_blocks[b].ff.layers[2].bias,\n","            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n","\n","        gpt.trf_blocks[b].norm1.scale = assign(\n","            gpt.trf_blocks[b].norm1.scale,\n","            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n","        gpt.trf_blocks[b].norm1.shift = assign(\n","            gpt.trf_blocks[b].norm1.shift,\n","            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n","        gpt.trf_blocks[b].norm2.scale = assign(\n","            gpt.trf_blocks[b].norm2.scale,\n","            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n","        gpt.trf_blocks[b].norm2.shift = assign(\n","            gpt.trf_blocks[b].norm2.shift,\n","            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n","\n","    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n","    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n","    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n","\n","\n","class LayerNorm(nn.Module):\n","    def __init__(self, emb_dim):\n","        super().__init__()\n","        self.eps = 1e-5\n","        self.scale = nn.Parameter(torch.ones(emb_dim))\n","        self.shift = nn.Parameter(torch.zeros(emb_dim))\n","\n","    def forward(self, x):\n","        mean = x.mean(dim=-1, keepdim=True)\n","        var = x.var(dim=-1, keepdim=True, unbiased=False)\n","        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n","        return self.scale * norm_x + self.shift\n","\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.att = MultiHeadAttention(\n","            d_in=cfg[\"emb_dim\"],\n","            d_out=cfg[\"emb_dim\"],\n","            context_length=cfg[\"context_length\"],\n","            num_heads=cfg[\"n_heads\"],\n","            dropout=cfg[\"drop_rate\"],\n","            qkv_bias=cfg[\"qkv_bias\"])\n","        self.ff = FeedForward(cfg)\n","        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n","        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n","        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n","\n","    def forward(self, x):\n","        # Shortcut connection for attention block\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut  # Add the original input back\n","\n","        # Shortcut connection for feed forward block\n","        shortcut = x\n","        x = self.norm2(x)\n","        x = self.ff(x)\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut  # Add the original input back\n","        return x\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n","            GELU(),\n","            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n","        )\n","\n","    def forward(self, x):\n","        return self.layers(x)\n","\n","\n","class GELU(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        return 0.5 * x * (1 + torch.tanh(\n","            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n","            (x + 0.044715 * torch.pow(x, 3))\n","        ))\n","\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n","        super().__init__()\n","        assert (d_out % num_heads == 0), \\\n","            \"d_out must be divisible by num_heads\"\n","\n","        self.d_out = d_out\n","        self.num_heads = num_heads\n","        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n","\n","        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer(\n","            \"mask\",\n","            torch.triu(torch.ones(context_length, context_length),\n","                       diagonal=1)\n","        )\n","\n","    def forward(self, x):\n","        b, num_tokens, d_in = x.shape\n","        # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`,\n","        # this will result in errors in the mask creation further below.\n","        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs\n","        # do not exceed `context_length` before reaching this forwar\n","\n","        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n","        queries = self.W_query(x)\n","        values = self.W_value(x)\n","\n","        # We implicitly split the matrix by adding a `num_heads` dimension\n","        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n","        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n","        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n","        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n","\n","        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n","        keys = keys.transpose(1, 2)\n","        queries = queries.transpose(1, 2)\n","        values = values.transpose(1, 2)\n","\n","        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n","        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n","\n","        # Original mask truncated to the number of tokens and converted to boolean\n","        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n","\n","        # Use the mask to fill attention scores\n","        attn_scores.masked_fill_(mask_bool, -torch.inf)\n","\n","        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","        attn_weights = self.dropout(attn_weights)\n","\n","        # Shape: (b, num_tokens, num_heads, head_dim)\n","        context_vec = (attn_weights @ values).transpose(1, 2)\n","\n","        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n","        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n","        context_vec = self.out_proj(context_vec) # optional projection\n","\n","        return context_vec\n","\n","\n","class GPTModel(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n","        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n","        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n","\n","        self.trf_blocks = nn.Sequential(\n","            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n","\n","        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n","        self.out_head = nn.Linear(\n","            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n","        )\n","\n","\n","    def forward(self, in_idx):\n","        batch_size, seq_len = in_idx.shape\n","        tok_embeds = self.tok_emb(in_idx)\n","        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n","        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_emb(x)\n","        x = self.trf_blocks(x)\n","        x = self.final_norm(x)\n","        logits = self.out_head(x)\n","        return logits\n","\n","\n","\n","BASE_CONFIG = {\n","    \"vocab_size\": 50257,     # Vocabulary size\n","    \"context_length\": 1024,  # Context length\n","    \"drop_rate\": 0.0,        # Dropout rate\n","    \"qkv_bias\": True         # Query-key-value bias\n","}\n","\n","model_configs = {\n","    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n","    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n","    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n","    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n","}\n","\n","CHOOSE_MODEL = \"gpt2-medium (355M)\"\n","\n","BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n","\n","model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n","settings, params = download_and_load_gpt2(\n","    model_size=model_size,\n","    models_dir=\"gpt2\"\n",")\n","\n","model = GPTModel(BASE_CONFIG)\n","load_weights_into_gpt(model, params)\n","model.eval();"],"metadata":{"id":"3yYD693XbzBG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749764795540,"user_tz":-540,"elapsed":89185,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"09f7ff09-9fd2-410b-e00f-ce219678d43a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 179kiB/s]\n","encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 5.45MiB/s]\n","hparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 197kiB/s]\n","model.ckpt.data-00000-of-00001: 100%|██████████| 1.42G/1.42G [01:19<00:00, 17.9MiB/s]\n","model.ckpt.index: 100%|██████████| 10.4k/10.4k [00:00<00:00, 21.4MiB/s]\n","model.ckpt.meta: 100%|██████████| 927k/927k [00:00<00:00, 5.01MiB/s]\n","vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 2.53MiB/s]\n"]}]},{"cell_type":"code","source":["import torch\n","torch.manual_seed(123)\n","\n","import json\n","import os\n","import urllib\n","\n","\n","def download_and_load_file(file_path, url):\n","\n","    if not os.path.exists(file_path):\n","        with urllib.request.urlopen(url) as response:\n","            text_data = response.read().decode(\"utf-8\")\n","        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n","            file.write(text_data)\n","\n","    # The book originally contained this unnecessary \"else\" clause:\n","    #else:\n","    #    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","    #        text_data = file.read()\n","\n","    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","        data = json.load(file)\n","\n","    return data\n","\n","\n","\n","file_path = \"instruction-data.json\"\n","url = (\n","    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n","    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",")\n","\n","data = download_and_load_file(file_path, url)\n","\n","\n","train_portion = int(len(data) * 0.85)  # 85% for training\n","test_portion = int(len(data) * 0.1)    # 10% for testing\n","val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n","\n","train_data = data[:train_portion]\n","test_data = data[train_portion:train_portion + test_portion]\n","val_data = data[train_portion + test_portion:]\n","\n","def format_input(entry):\n","    instruction_text = (\n","        f\"Below is an instruction that describes a task. \"\n","        f\"Write a response that appropriately completes the request.\"\n","        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n","    )\n","\n","    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n","\n","    return instruction_text + input_text\n","\n","\n","\n","input_text = format_input(val_data[0])\n","print(input_text)"],"metadata":{"id":"if5gUEX6cSSK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749973963167,"user_tz":-540,"elapsed":3905,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"53105bbb-5fbc-48d8-8a8c-1f251ae9362a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"]}]},{"cell_type":"code","source":["#from previous_chapters import (\n","#    generate,\n","#    text_to_token_ids,\n","#    token_ids_to_text\n","#)\n","# Alternatively:\n","# from llms_from_scratch.ch05 import (\n","#    generate_text_simple,\n","#    text_to_token_ids,\n","#    token_ids_to_text\n","# )\n","\n","import tiktoken\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n","\n","    # For-loop is the same as before: Get logits, and only focus on last time step\n","    for _ in range(max_new_tokens):\n","        idx_cond = idx[:, -context_size:]\n","        with torch.no_grad():\n","            logits = model(idx_cond)\n","        logits = logits[:, -1, :]\n","\n","        # New: Filter logits with top_k sampling\n","        if top_k is not None:\n","            # Keep only top_k values\n","            top_logits, _ = torch.topk(logits, top_k)\n","            min_val = top_logits[:, -1]\n","            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n","\n","        # New: Apply temperature scaling\n","        if temperature > 0.0:\n","            logits = logits / temperature\n","\n","            # Apply softmax to get probabilities\n","            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n","\n","            # Sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n","\n","        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n","        else:\n","            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n","\n","        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n","            break\n","\n","        # Same as before: append sampled index to the running sequence\n","        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n","\n","    return idx\n","\n","def text_to_token_ids(text, tokenizer):\n","    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n","    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n","    return encoded_tensor\n","\n","def token_ids_to_text(token_ids, tokenizer):\n","    flat = token_ids.squeeze(0) # remove batch dimension\n","    return tokenizer.decode(flat.tolist())\n","\n","\n","token_ids = generate(\n","    model=model,\n","    idx=text_to_token_ids(input_text, tokenizer),\n","    max_new_tokens=35,\n","    context_size=BASE_CONFIG[\"context_length\"],\n","    eos_id=50256,\n",")\n","generated_text = token_ids_to_text(token_ids, tokenizer)"],"metadata":{"id":"C6hlPwFccbV4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response_text = (\n","    generated_text[len(input_text):]\n","    .replace(\"### Response:\", \"\")\n","    .strip()\n",")\n","print(response_text)"],"metadata":{"id":"IxnxVlq3clPA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749680080240,"user_tz":-540,"elapsed":9,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"073f0766-6d13-49c6-90c9-f4ded47f7bac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The chef cooks the meal every day.\n","\n","### Instruction:\n","\n","Convert the active sentence to passive: 'The chef cooks the\n"]}]},{"cell_type":"markdown","source":["7.6 Finetuning the LLM on instruction data"],"metadata":{"id":"dd1pOAKAc2q5"}},{"cell_type":"code","source":["#from previous_chapters import (\n","#    calc_loss_loader,\n","#    train_model_simple\n","#)\n","# Alternatively:\n","# from llms_from_scratch.ch05 import (\n","#    calc_loss_loader,\n","#    train_model_simple,\n","# )"],"metadata":{"id":"Pi_C3b_AcsfH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","import tiktoken\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model.to(device)\n","\n","torch.manual_seed(123)\n","\n","def calc_loss_batch(input_batch, target_batch, model, device):\n","    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n","    logits = model(input_batch)\n","    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n","    return loss\n","\n","def calc_loss_loader(data_loader, model, device, num_batches=None):\n","    total_loss = 0.\n","    if len(data_loader) == 0:\n","        return float(\"nan\")\n","    elif num_batches is None:\n","        num_batches = len(data_loader)\n","    else:\n","        # Reduce the number of batches to match the total number of batches in the data loader\n","        # if num_batches exceeds the number of batches in the data loader\n","        num_batches = min(num_batches, len(data_loader))\n","    for i, (input_batch, target_batch) in enumerate(data_loader):\n","        if i < num_batches:\n","            loss = calc_loss_batch(input_batch, target_batch, model, device)\n","            total_loss += loss.item()\n","        else:\n","            break\n","    return total_loss / num_batches\n","\n","class InstructionDataset(Dataset):\n","    def __init__(self, data, tokenizer):\n","        self.data = data\n","\n","        # Pre-tokenize texts\n","        self.encoded_texts = []\n","        for entry in data:\n","            instruction_plus_input = format_input(entry)\n","            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n","            full_text = instruction_plus_input + response_text\n","            self.encoded_texts.append(\n","                tokenizer.encode(full_text)\n","            )\n","\n","    def __getitem__(self, index):\n","        return self.encoded_texts[index]\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","num_workers = 0\n","batch_size = 8\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","from functools import partial\n","\n","customized_collate_fn = partial(\n","    #custom_collate_fn,\n","    allowed_max_length=512,\n","    device=device\n",")\n","\"\"\"\n","def customized_collate_fn(batch):\n","    for item in batch:\n","        print(type(item), len(item), item)\n","    # 各サンプルの入力とラベルを分離\n","    #inputs, labels = zip(*batch)\n","    inputs = zip(*batch)\n","    # パディング処理を実施（異なる長さのテンソルに対応）\n","    inputs_padded = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n","    # テンソル化\n","    labels = torch.tensor(labels)\n","    return inputs_padded, labels\n","\n","\n","train_dataset = InstructionDataset(train_data, tokenizer)\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=batch_size,\n","    collate_fn=customized_collate_fn,#不明です2025/06/12\n","    shuffle=True,\n","    drop_last=True,\n","    num_workers=num_workers\n",")\n","\"\"\"\n","with torch.no_grad():\n","    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n","    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n","\n","print(\"Training loss:\", train_loss)\n","print(\"Validation loss:\", val_loss)"],"metadata":{"id":"XOcA2FFBde8O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749766493381,"user_tz":-540,"elapsed":1526,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"d21f134b-2add-4b7d-ebe6-9f21bc8ce53e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training loss: 3.438487005233765\n","Validation loss: 3.458932065963745\n"]}]},{"cell_type":"code","source":["import time\n","import torch\n","\n","start_time = time.time()\n","\n","torch.manual_seed(123)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n","\n","num_epochs = 2\n","\n","def calc_loss_batch(input_batch, target_batch, model, device):\n","    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n","    logits = model(input_batch)\n","    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n","    return loss\n","\n","def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n","                       eval_freq, eval_iter, start_context, tokenizer):\n","    # Initialize lists to track losses and tokens seen\n","    train_losses, val_losses, track_tokens_seen = [], [], []\n","    tokens_seen, global_step = 0, -1\n","\n","    # Main training loop\n","    for epoch in range(num_epochs):\n","        model.train()  # Set model to training mode\n","\n","        for input_batch, target_batch in train_loader:\n","            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n","            loss = calc_loss_batch(input_batch, target_batch, model, device)\n","            loss.backward() # Calculate loss gradients\n","            optimizer.step() # Update model weights using loss gradients\n","            tokens_seen += input_batch.numel()\n","            global_step += 1\n","\n","            # Optional evaluation step\n","            if global_step % eval_freq == 0:\n","                train_loss, val_loss = evaluate_model(\n","                    model, train_loader, val_loader, device, eval_iter)\n","                train_losses.append(train_loss)\n","                val_losses.append(val_loss)\n","                track_tokens_seen.append(tokens_seen)\n","                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n","                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n","\n","train_losses, val_losses, tokens_seen = train_model_simple(\n","    model, train_loader, val_loader, optimizer, device,\n","    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n","    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",")\n","\n","end_time = time.time()\n","execution_time_minutes = (end_time - start_time) / 60\n","print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"],"metadata":{"id":"fB41yDgwdm5e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749767091453,"user_tz":-540,"elapsed":48176,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"b78faa04-7901-43a3-903f-1edf4c07f060"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ep 1 (Step 000000): Train loss 0.145, Val loss 0.676\n","Ep 1 (Step 000005): Train loss 0.188, Val loss 0.751\n","Ep 1 (Step 000010): Train loss 0.158, Val loss 0.761\n","Ep 1 (Step 000015): Train loss 0.172, Val loss 0.740\n","Ep 1 (Step 000020): Train loss 0.169, Val loss 0.709\n","Ep 1 (Step 000025): Train loss 0.161, Val loss 0.675\n","Ep 1 (Step 000030): Train loss 0.154, Val loss 0.658\n","Ep 1 (Step 000035): Train loss 0.168, Val loss 0.664\n","Ep 1 (Step 000040): Train loss 0.157, Val loss 0.670\n","Ep 1 (Step 000045): Train loss 0.161, Val loss 0.682\n","Ep 1 (Step 000050): Train loss 0.135, Val loss 0.695\n","Ep 1 (Step 000055): Train loss 0.154, Val loss 0.693\n","Ep 1 (Step 000060): Train loss 0.172, Val loss 0.669\n","Ep 1 (Step 000065): Train loss 0.163, Val loss 0.661\n","Ep 1 (Step 000070): Train loss 0.158, Val loss 0.674\n","Ep 1 (Step 000075): Train loss 0.168, Val loss 0.695\n","Ep 1 (Step 000080): Train loss 0.145, Val loss 0.701\n","Ep 1 (Step 000085): Train loss 0.150, Val loss 0.693\n","Ep 1 (Step 000090): Train loss 0.139, Val loss 0.675\n","Ep 1 (Step 000095): Train loss 0.148, Val loss 0.661\n","Ep 1 (Step 000100): Train loss 0.142, Val loss 0.651\n","Ep 1 (Step 000105): Train loss 0.144, Val loss 0.657\n","Ep 1 (Step 000110): Train loss 0.145, Val loss 0.660\n","Ep 1 (Step 000115): Train loss 0.150, Val loss 0.674\n","Ep 2 (Step 000120): Train loss 0.141, Val loss 0.676\n","Ep 2 (Step 000125): Train loss 0.143, Val loss 0.711\n","Ep 2 (Step 000130): Train loss 0.162, Val loss 0.733\n","Ep 2 (Step 000135): Train loss 0.134, Val loss 0.740\n","Ep 2 (Step 000140): Train loss 0.146, Val loss 0.734\n","Ep 2 (Step 000145): Train loss 0.157, Val loss 0.718\n","Ep 2 (Step 000150): Train loss 0.137, Val loss 0.690\n","Ep 2 (Step 000155): Train loss 0.165, Val loss 0.680\n","Ep 2 (Step 000160): Train loss 0.136, Val loss 0.687\n","Ep 2 (Step 000165): Train loss 0.139, Val loss 0.693\n","Ep 2 (Step 000170): Train loss 0.143, Val loss 0.686\n","Ep 2 (Step 000175): Train loss 0.138, Val loss 0.674\n","Ep 2 (Step 000180): Train loss 0.146, Val loss 0.667\n","Ep 2 (Step 000185): Train loss 0.153, Val loss 0.664\n","Ep 2 (Step 000190): Train loss 0.143, Val loss 0.665\n","Ep 2 (Step 000195): Train loss 0.126, Val loss 0.670\n","Ep 2 (Step 000200): Train loss 0.140, Val loss 0.673\n","Ep 2 (Step 000205): Train loss 0.135, Val loss 0.658\n","Ep 2 (Step 000210): Train loss 0.134, Val loss 0.659\n","Ep 2 (Step 000215): Train loss 0.146, Val loss 0.675\n","Ep 2 (Step 000220): Train loss 0.125, Val loss 0.701\n","Ep 2 (Step 000225): Train loss 0.128, Val loss 0.717\n","Ep 2 (Step 000230): Train loss 0.135, Val loss 0.718\n","Training completed in 0.80 minutes.\n"]}]},{"cell_type":"code","source":["import time\n","import torch\n","def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n","    model.eval()\n","    with torch.no_grad():\n","        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n","        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n","    model.train()\n","    return train_loss, val_loss\n","\n","def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n","                       eval_freq, eval_iter, start_context, tokenizer):\n","    # Initialize lists to track losses and tokens seen\n","    train_losses, val_losses, track_tokens_seen = [], [], []\n","    tokens_seen, global_step = 0, -1\n","\n","    # Main training loop\n","    for epoch in range(num_epochs):\n","        model.train()  # Set model to training mode\n","\n","        for input_batch, target_batch in train_loader:\n","            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n","            loss = calc_loss_batch(input_batch, target_batch, model, device)\n","            loss.backward() # Calculate loss gradients\n","            optimizer.step() # Update model weights using loss gradients\n","            tokens_seen += input_batch.numel()\n","            global_step += 1\n","\n","            # Optional evaluation step\n","            if global_step % eval_freq == 0:\n","                train_loss, val_loss = evaluate_model(\n","                    model, train_loader, val_loader, device, eval_iter)\n","                train_losses.append(train_loss)\n","                val_losses.append(val_loss)\n","                track_tokens_seen.append(tokens_seen)\n","                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n","                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n","\n","def generate_text_simple(model, idx, max_new_tokens, context_size):\n","    # idx is (batch, n_tokens) array of indices in the current context\n","    for _ in range(max_new_tokens):\n","\n","        # Crop current context if it exceeds the supported context size\n","        # E.g., if LLM supports only 5 tokens, and the context size is 10\n","        # then only the last 5 tokens are used as context\n","        idx_cond = idx[:, -context_size:]\n","\n","        # Get the predictions\n","        with torch.no_grad():\n","            logits = model(idx_cond)\n","\n","        # Focus only on the last time step\n","        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n","        logits = logits[:, -1, :]\n","\n","        # Apply softmax to get probabilities\n","        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n","\n","        # Get the idx of the vocab entry with the highest probability value\n","        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n","\n","        # Append sampled index to the running sequence\n","        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n","\n","    return idx\n","\n","\n","start_time = time.time()\n","\n","torch.manual_seed(123)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n","\n","num_epochs = 2\n","\n","train_losses, val_losses, tokens_seen = train_model_simple(\n","    model, train_loader, val_loader, optimizer, device,\n","    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n","    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",")\n","\n","end_time = time.time()\n","execution_time_minutes = (end_time - start_time) / 60\n","print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1749771689615,"user_tz":-540,"elapsed":47859,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"9754d098-431c-4425-8e78-2deb5bfa3329","id":"s-LNz6FZ23_w"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ep 1 (Step 000000): Train loss 0.132, Val loss 0.734\n","Ep 1 (Step 000005): Train loss 0.140, Val loss 0.784\n","Ep 1 (Step 000010): Train loss 0.132, Val loss 0.800\n","Ep 1 (Step 000015): Train loss 0.154, Val loss 0.834\n","Ep 1 (Step 000020): Train loss 0.162, Val loss 0.828\n","Ep 1 (Step 000025): Train loss 0.154, Val loss 0.804\n","Ep 1 (Step 000030): Train loss 0.142, Val loss 0.750\n","Ep 1 (Step 000035): Train loss 0.159, Val loss 0.721\n","Ep 1 (Step 000040): Train loss 0.154, Val loss 0.713\n","Ep 1 (Step 000045): Train loss 0.154, Val loss 0.720\n","Ep 1 (Step 000050): Train loss 0.134, Val loss 0.731\n","Ep 1 (Step 000055): Train loss 0.137, Val loss 0.723\n","Ep 1 (Step 000060): Train loss 0.136, Val loss 0.720\n","Ep 1 (Step 000065): Train loss 0.135, Val loss 0.722\n","Ep 1 (Step 000070): Train loss 0.143, Val loss 0.725\n","Ep 1 (Step 000075): Train loss 0.144, Val loss 0.726\n","Ep 1 (Step 000080): Train loss 0.132, Val loss 0.751\n","Ep 1 (Step 000085): Train loss 0.141, Val loss 0.779\n","Ep 1 (Step 000090): Train loss 0.123, Val loss 0.769\n","Ep 1 (Step 000095): Train loss 0.151, Val loss 0.757\n","Ep 1 (Step 000100): Train loss 0.133, Val loss 0.750\n","Ep 1 (Step 000105): Train loss 0.139, Val loss 0.738\n","Ep 1 (Step 000110): Train loss 0.145, Val loss 0.729\n","Ep 1 (Step 000115): Train loss 0.134, Val loss 0.727\n","Ep 2 (Step 000120): Train loss 0.138, Val loss 0.744\n","Ep 2 (Step 000125): Train loss 0.137, Val loss 0.774\n","Ep 2 (Step 000130): Train loss 0.137, Val loss 0.796\n","Ep 2 (Step 000135): Train loss 0.128, Val loss 0.816\n","Ep 2 (Step 000140): Train loss 0.142, Val loss 0.833\n","Ep 2 (Step 000145): Train loss 0.137, Val loss 0.845\n","Ep 2 (Step 000150): Train loss 0.123, Val loss 0.840\n","Ep 2 (Step 000155): Train loss 0.153, Val loss 0.823\n","Ep 2 (Step 000160): Train loss 0.127, Val loss 0.801\n","Ep 2 (Step 000165): Train loss 0.127, Val loss 0.772\n","Ep 2 (Step 000170): Train loss 0.135, Val loss 0.746\n","Ep 2 (Step 000175): Train loss 0.139, Val loss 0.738\n","Ep 2 (Step 000180): Train loss 0.128, Val loss 0.740\n","Ep 2 (Step 000185): Train loss 0.141, Val loss 0.734\n","Ep 2 (Step 000190): Train loss 0.129, Val loss 0.734\n","Ep 2 (Step 000195): Train loss 0.118, Val loss 0.729\n","Ep 2 (Step 000200): Train loss 0.131, Val loss 0.728\n","Ep 2 (Step 000205): Train loss 0.134, Val loss 0.712\n","Ep 2 (Step 000210): Train loss 0.118, Val loss 0.713\n","Ep 2 (Step 000215): Train loss 0.132, Val loss 0.718\n","Ep 2 (Step 000220): Train loss 0.118, Val loss 0.731\n","Ep 2 (Step 000225): Train loss 0.123, Val loss 0.748\n","Ep 2 (Step 000230): Train loss 0.127, Val loss 0.754\n"]},{"output_type":"error","ename":"TypeError","evalue":"cannot unpack non-iterable NoneType object","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-43-1405492879>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m train_losses, val_losses, tokens_seen = train_model_simple(\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from matplotlib.ticker import MaxNLocator\n","#from previous_chapters import plot_losses\n","# Alternatively:\n","# from llms_from_scratch.ch05 import plot_losses\n","def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n","    fig, ax1 = plt.subplots(figsize=(5, 3))\n","\n","    # Plot training and validation loss against epochs\n","    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n","    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n","    ax1.set_xlabel(\"Epochs\")\n","    ax1.set_ylabel(\"Loss\")\n","    ax1.legend(loc=\"upper right\")\n","    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n","\n","    # Create a second x-axis for tokens seen\n","    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n","    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n","    ax2.set_xlabel(\"Tokens seen\")\n","\n","    fig.tight_layout()  # Adjust layout to make room\n","    plt.savefig(\"loss-plot.pdf\")\n","    plt.show()\n","\n","epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n","plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"],"metadata":{"id":"DNONv8-TdzFd","colab":{"base_uri":"https://localhost:8080/","height":307},"executionInfo":{"status":"ok","timestamp":1749771990858,"user_tz":-540,"elapsed":1050,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"3eaa8f85-90a6-4200-d678-d821c1c63520"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 500x300 with 2 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["7.7 Extracting and saving responses"],"metadata":{"id":"pU7r5V5OeKR3"}},{"cell_type":"code","source":["torch.manual_seed(123)\n","\n","def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n","\n","    # For-loop is the same as before: Get logits, and only focus on last time step\n","    for _ in range(max_new_tokens):\n","        idx_cond = idx[:, -context_size:]\n","        with torch.no_grad():\n","            logits = model(idx_cond)\n","        logits = logits[:, -1, :]\n","\n","        # New: Filter logits with top_k sampling\n","        if top_k is not None:\n","            # Keep only top_k values\n","            top_logits, _ = torch.topk(logits, top_k)\n","            min_val = top_logits[:, -1]\n","            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n","\n","        # New: Apply temperature scaling\n","        if temperature > 0.0:\n","            logits = logits / temperature\n","\n","            # Apply softmax to get probabilities\n","            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n","\n","            # Sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n","\n","        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n","        else:\n","            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n","\n","        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n","            break\n","\n","        # Same as before: append sampled index to the running sequence\n","        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n","\n","    return idx\n","\n","def text_to_token_ids(text, tokenizer):\n","    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n","    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n","    return encoded_tensor\n","\n","def token_ids_to_text(token_ids, tokenizer):\n","    flat = token_ids.squeeze(0) # remove batch dimension\n","    return tokenizer.decode(flat.tolist())\n","\n","for entry in test_data[:3]:\n","    input_text = format_input(entry)\n","    token_ids = generate(\n","        model=model,\n","        idx=text_to_token_ids(input_text, tokenizer).to(device),\n","        max_new_tokens=256,\n","        context_size=BASE_CONFIG[\"context_length\"],\n","        eos_id=50256\n","    )\n","    generated_text = token_ids_to_text(token_ids, tokenizer)\n","    response_text = (\n","        generated_text[len(input_text):]\n","        .replace(\"### Response:\", \"\")\n","        .strip()\n",")\n","\n","    print(input_text)\n","    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n","    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n","    print(\"-------------------------------------\")"],"metadata":{"id":"lgpV7fBjeMxw","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"error","timestamp":1749974183021,"user_tz":-540,"elapsed":16,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"d6727a27-dcc2-4bda-e1f1-8f66ca70905b"},"execution_count":14,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'tokenizer' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-907320792>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m     token_ids = generate(\n\u001b[1;32m     53\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_to_token_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mcontext_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBASE_CONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"context_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"]}]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n","\n","    input_text = format_input(entry)\n","\n","    token_ids = generate(\n","        model=model,\n","        idx=text_to_token_ids(input_text, tokenizer).to(device),\n","        max_new_tokens=256,\n","        context_size=BASE_CONFIG[\"context_length\"],\n","        eos_id=50256\n","    )\n","    generated_text = token_ids_to_text(token_ids, tokenizer)\n","    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n","\n","    test_data[i][\"model_response\"] = response_text\n","\n","\n","with open(\"instruction-data-with-response.json\", \"w\") as file:\n","    json.dump(test_data, file, indent=4)  # \"indent\" for pretty-printing"],"metadata":{"id":"wE2gKmFueCtG","colab":{"base_uri":"https://localhost:8080/","height":240},"executionInfo":{"status":"error","timestamp":1749974130399,"user_tz":-540,"elapsed":18,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"e2cae0e3-bce7-4e3a-ad87-8145d5fe1b4d"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/110 [00:00<?, ?it/s]\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'generate' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-3171075598>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     token_ids = generate(\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_to_token_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'generate' is not defined"]}]},{"cell_type":"code","source":["print(test_data[0])"],"metadata":{"id":"rlECLfTZeskF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749773324975,"user_tz":-540,"elapsed":49,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"6474fbf0-93f9-4d10-f418-a942bd8db07a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'instruction': 'Rewrite the sentence using a simile.', 'input': 'The car is very fast.', 'output': 'The car is as fast as lightning.', 'model_response': 'The car is as fast as a cheetah.\\n\\n### Instruction:\\nThe car raced down the road.\\n\\n\\nThe car raced down the road like a bullet.\\n\\n### Input:\\nThe car raced down the road like a bullet.\\n\\n\\nThe car raced down the road like a bullet.\\n\\n### Input:\\nThe car raced down the road like a bullet.\\n\\n\\nThe car raced down the road like a bullet.\\n\\n### Input:\\nThe car raced down the road like a bullet.\\n\\n\\nThe car raced down the road like a bullet.\\n\\n### Input:\\nThe car raced down the road like a bullet.\\n\\n\\nThe car raced down the road like a bullet.\\n\\n### Input:\\nThe car raced down the road like a bullet.\\n\\n\\nThe car raced down the road like a bullet.\\n\\n### Input:\\nThe car raced down the road like a bullet.\\n\\n\\nThe car raced down the road like a bullet.\\n\\n### Input:\\nThe car raced down the road like a bullet.'}\n"]}]},{"cell_type":"code","source":["import re\n","\n","file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"\n","torch.save(model.state_dict(), file_name)\n","print(f\"Model saved as {file_name}\")\n","\n","# Load model via\n","# model.load_state_dict(torch.load(\"gpt2-medium355M-sft.pth\"))"],"metadata":{"id":"1xkm9B5Eezd0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749773330527,"user_tz":-540,"elapsed":2367,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"74bb8791-dec0-4db3-af95-f90c1ef5956d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model saved as gpt2-medium355M-sft.pth\n"]}]},{"cell_type":"markdown","source":["7.8 Evaluating the finetuned LLM"],"metadata":{"id":"PSaI8t8zfFIr"}},{"cell_type":"code","source":["!ollama"],"metadata":{"id":"u7f1qjqPfv-9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import psutil\n","\n","# 実行中のプロセス一覧を表示\n","print([p.info[\"name\"] for p in psutil.process_iter([\"name\"])])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wPXm3P5kfvkd","executionInfo":{"status":"ok","timestamp":1749956579373,"user_tz":-540,"elapsed":55,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"090929d4-dd63-4171-9b0b-0786ec4d7eed"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["['docker-init', 'node', 'oom_monitor.sh', 'run.sh', 'kernel_manager_proxy', 'tail', 'tail', 'python3', 'colab-fileshim.', 'jupyter-noteboo', 'dap_multiplexer', 'python3', 'python3', 'language_service', 'node', 'sleep']\n"]}]},{"cell_type":"code","source":["import psutil\n","\n","def check_if_running(process_name):\n","    running = False\n","    for proc in psutil.process_iter([\"name\"]):\n","        if process_name in proc.info[\"name\"]:\n","            running = True\n","            break\n","    return running\n","\n","ollama_running = check_if_running(\"ollama\")\n","\n","if not ollama_running:\n","    raise RuntimeError(\"Ollama not running. Launch ollama before proceeding.\")\n","print(\"Ollama running:\", check_if_running(\"ollama\"))"],"metadata":{"id":"UcmA7FUxe8kd","colab":{"base_uri":"https://localhost:8080/","height":203},"executionInfo":{"status":"error","timestamp":1749955780601,"user_tz":-540,"elapsed":536,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"c0828ab2-fae6-4a82-98d4-d2b3f04f074c"},"execution_count":1,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Ollama not running. Launch ollama before proceeding.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-242962959>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mollama_running\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ollama not running. Launch ollama before proceeding.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ollama running:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_if_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ollama\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Ollama not running. Launch ollama before proceeding."]}]},{"cell_type":"code","source":["# This cell is optional; it allows you to restart the notebook\n","# and only run section 7.7 without rerunning any of the previous code\n","import json\n","from tqdm import tqdm\n","\n","file_path = \"instruction-data-with-response.json\"\n","\n","with open(file_path, \"r\") as file:\n","    test_data = json.load(file)\n","\n","\n","def format_input(entry):\n","    instruction_text = (\n","        f\"Below is an instruction that describes a task. \"\n","        f\"Write a response that appropriately completes the request.\"\n","        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n","    )\n","\n","    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n","\n","    return instruction_text + input_text"],"metadata":{"id":"2-8j1v4OfiUP","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"error","timestamp":1749966192812,"user_tz":-540,"elapsed":40,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"efcb4cb2-2933-4d61-c73a-f876a2de2937"},"execution_count":1,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'instruction-data-with-response.json'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-2346499000>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"instruction-data-with-response.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'instruction-data-with-response.json'"]}]},{"cell_type":"code","source":["import urllib.request\n","\n","def query_model(\n","    prompt,\n","    model=\"llama3\",\n","    url=\"http://localhost:11434/api/chat\"\n","):\n","    # Create the data payload as a dictionary\n","    data = {\n","        \"model\": model,\n","        \"messages\": [\n","            {\"role\": \"user\", \"content\": prompt}\n","        ],\n","        \"options\": {     # Settings below are required for deterministic responses\n","            \"seed\": 123,\n","            \"temperature\": 0,\n","            \"num_ctx\": 2048\n","        }\n","    }\n","\n","\n","    # Convert the dictionary to a JSON formatted string and encode it to bytes\n","    payload = json.dumps(data).encode(\"utf-8\")\n","\n","    # Create a request object, setting the method to POST and adding necessary headers\n","    request = urllib.request.Request(\n","        url,\n","        data=payload,\n","        method=\"POST\"\n","    )\n","    request.add_header(\"Content-Type\", \"application/json\")\n","\n","    # Send the request and capture the response\n","    response_data = \"\"\n","    with urllib.request.urlopen(request) as response:\n","        # Read and decode the response\n","        while True:\n","            line = response.readline().decode(\"utf-8\")\n","            if not line:\n","                break\n","            response_json = json.loads(line)\n","            response_data += response_json[\"message\"][\"content\"]\n","\n","    return response_data\n","\n","\n","model = \"llama3\"\n","result = query_model(\"What do Llamas eat?\", model)\n","print(result)"],"metadata":{"id":"z6lmwhCNfsRs","colab":{"base_uri":"https://localhost:8080/","height":429},"executionInfo":{"status":"error","timestamp":1749968394034,"user_tz":-540,"elapsed":10,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"d0de8d8a-c6cd-4fb2-c6d5-9dc32b2a9b56"},"execution_count":2,"outputs":[{"output_type":"error","ename":"URLError","evalue":"<urlopen error [Errno 99] Cannot assign requested address>","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1347\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0m\u001b[1;32m   1349\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n","\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1057\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    995\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    997\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    961\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http.client.connect\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m         self.sock = self._create_connection(\n\u001b[0m\u001b[1;32m    963\u001b[0m             (self.host,self.port), self.timeout, self.source_address)\n","\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mExceptionGroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"create_connection failed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[1;32m    847\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 99] Cannot assign requested address","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-3923282217>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"llama3\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What do Llamas eat?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-3923282217>\u001b[0m in \u001b[0;36mquery_model\u001b[0;34m(prompt, model, url)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Send the request and capture the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mresponse_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Read and decode the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'urllib.Request'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[1;32m    537\u001b[0m                                   '_open', req)\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mhttp_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPConnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m     \u001b[0mhttp_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[1;32m   1350\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1351\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1352\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno 99] Cannot assign requested address>"]}]},{"cell_type":"code","source":["model = \"llama3\"\n","result = query_model(\"What do Llamas eat?\", model)\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":429},"id":"sVIIxZo_ebad","executionInfo":{"status":"error","timestamp":1749972316263,"user_tz":-540,"elapsed":32,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"731fbac6-8c35-4d02-fc11-a1e17fa60877"},"execution_count":5,"outputs":[{"output_type":"error","ename":"URLError","evalue":"<urlopen error [Errno 99] Cannot assign requested address>","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1347\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0m\u001b[1;32m   1349\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n","\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1057\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    995\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    997\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    961\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http.client.connect\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m         self.sock = self._create_connection(\n\u001b[0m\u001b[1;32m    963\u001b[0m             (self.host,self.port), self.timeout, self.source_address)\n","\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mExceptionGroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"create_connection failed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[1;32m    847\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 99] Cannot assign requested address","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-2559162277>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"llama3\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What do Llamas eat?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-3923282217>\u001b[0m in \u001b[0;36mquery_model\u001b[0;34m(prompt, model, url)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Send the request and capture the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mresponse_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Read and decode the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'urllib.Request'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[1;32m    537\u001b[0m                                   '_open', req)\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mhttp_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPConnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m     \u001b[0mhttp_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[1;32m   1350\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1351\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1352\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno 99] Cannot assign requested address>"]}]},{"cell_type":"code","source":["for entry in test_data[:3]:\n","    prompt = (\n","        f\"Given the input `{format_input(entry)}` \"\n","        f\"and correct output `{entry['output']}`, \"\n","        f\"score the model response `{entry['model_response']}`\"\n","        f\" on a scale from 0 to 100, where 100 is the best score. \"\n","    )\n","    print(\"\\nDataset response:\")\n","    print(\">>\", entry['output'])\n","    print(\"\\nModel response:\")\n","    print(\">>\", entry[\"model_response\"])\n","    print(\"\\nScore:\")\n","    print(\">>\", query_model(prompt))\n","    print(\"\\n-------------------------\")"],"metadata":{"id":"u0qvHXCVf4bU","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"error","timestamp":1749974029499,"user_tz":-540,"elapsed":9,"user":{"displayName":"noriaki nakamura","userId":"03778223969126112353"}},"outputId":"a64e6d52-9d73-4513-aedf-2639b487d4cb"},"execution_count":11,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'model_response'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-648976550>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;34mf\"Given the input `{format_input(entry)}` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;34mf\"and correct output `{entry['output']}`, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;34mf\"score the model response `{entry['model_response']}`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;34mf\" on a scale from 0 to 100, where 100 is the best score. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     )\n","\u001b[0;31mKeyError\u001b[0m: 'model_response'"]}]},{"cell_type":"code","source":["def generate_model_scores(json_data, json_key, model=\"llama3\"):\n","    scores = []\n","    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n","        prompt = (\n","            f\"Given the input `{format_input(entry)}` \"\n","            f\"and correct output `{entry['output']}`, \"\n","            f\"score the model response `{entry[json_key]}`\"\n","            f\" on a scale from 0 to 100, where 100 is the best score. \"\n","            f\"Respond with the integer number only.\"\n","        )\n","        score = query_model(prompt, model)\n","        try:\n","            scores.append(int(score))\n","        except ValueError:\n","            print(f\"Could not convert score: {score}\")\n","            continue\n","\n","    return scores\n","\n","\n","scores = generate_model_scores(test_data, \"model_response\")\n","print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n","print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"],"metadata":{"id":"ZR79Fx0mgDpM"},"execution_count":null,"outputs":[]}]}